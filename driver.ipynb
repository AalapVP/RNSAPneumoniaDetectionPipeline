{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124103a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import ImageDraw, Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5642c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define transformation for training set\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "                         \n",
    "])\n",
    "\n",
    "#define transformation for validation set\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "                         \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec1a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"C:\\Users\\Aalap\\Documents\\ML\\rnsa\\training\"\n",
    "val_dir = r\"C:\\Users\\Aalap\\Documents\\ML\\rnsa\\validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0587cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root = train_dir, transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(root = val_dir, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643355b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['lung_opacity', 'no_lung_opacity_not_normal', 'normal'] \n",
      "Class to index mapping: {'lung_opacity': 0, 'no_lung_opacity_not_normal': 1, 'normal': 2}\n"
     ]
    }
   ],
   "source": [
    "classes = train_dataset.classes\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "print(f\"Class names: {classes} \\nClass to index mapping: {class_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325d0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcd1a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 4,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    num_workers = 4,\n",
    "    pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c85827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a computing device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fde21702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d290f2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\Aalap/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is read to be trained on device: cuda\n"
     ]
    }
   ],
   "source": [
    "#load pretrained model\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#get the numeber of input features of the last layer\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "#modify the last layer to match the number of classes in our dataset\n",
    "model.fc = nn.Linear(num_ftrs, 3) \n",
    "\n",
    "#move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model is read to be trained on device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b87c8c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7754\n",
      "Epoch [2/10], Loss: 0.7000\n",
      "Epoch [3/10], Loss: 0.6817\n",
      "Epoch [4/10], Loss: 0.6649\n",
      "Epoch [5/10], Loss: 0.6566\n",
      "Epoch [6/10], Loss: 0.6479\n",
      "Epoch [7/10], Loss: 0.6386\n",
      "Epoch [8/10], Loss: 0.6254\n",
      "Epoch [9/10], Loss: 0.6203\n",
      "Epoch [10/10], Loss: 0.6139\n"
     ]
    }
   ],
   "source": [
    "#define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #train\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        #set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    #print average loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ccb71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Instantiate the model architecture\n",
    "# num_classes = 3\n",
    "# model_classifier = models.resnet50()\n",
    "# num_ftrs = model_classifier.fc.in_features\n",
    "# model_classifier.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# # 2. Load the state dictionary from the saved file\n",
    "# model_classifier.load_state_dict(torch.load('resnet_classifier.pth'))\n",
    "\n",
    "# # Move the model to the appropriate device (CPU or GPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_classifier.to(device)\n",
    "\n",
    "# # Set the model to evaluation mode\n",
    "# model_classifier.eval()\n",
    "\n",
    "# print(\"Model has been loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3abfbc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6861\n",
      "Validation Accuracy: 68.30%\n"
     ]
    }
   ],
   "source": [
    "# Validation phase\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "val_loss = 0.0\n",
    "with torch.no_grad(): # Disable gradient calculation for evaluation\n",
    "    for images, labels in val_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print validation metrics\n",
    "print(f\"Validation Loss: {val_loss/len(val_dataloader):.4f}\")\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252992a",
   "metadata": {},
   "source": [
    "This model performance is only based on the images in training. This does not include the meta data yet. It can be useful to add meta data into the classification process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b3832",
   "metadata": {},
   "source": [
    "First fine tune the model for 3 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eea7dbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "##Loading the previously saved model\n",
    "\n",
    "# 1. Instantiate the model architecture\n",
    "num_classes = 3\n",
    "model_classifier = models.resnet50()\n",
    "num_ftrs = model_classifier.fc.in_features\n",
    "model_classifier.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# 2. Load the state dictionary from the saved file\n",
    "model_classifier.load_state_dict(torch.load('resnet_classifier.pth'))\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_classifier.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_classifier.eval()\n",
    "\n",
    "print(\"Model has been loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8adebbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "bn1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "relu ReLU(inplace=True)\n",
      "maxpool MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "layer1 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer2 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer3 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (4): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (5): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer4 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "avgpool AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "fc Linear(in_features=2048, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "#check the model layers\n",
    "for name, layer in model_classifier.named_children():\n",
    "    print(name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80a7c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting all the layers to not require gradients\n",
    "#freezing layers\n",
    "for param in model_classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#unfreezing the final layer\n",
    "for param in model_classifier.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#unfreezing the layer 4 parameters\n",
    "for param in model_classifier.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#define a new optimizer that includes the unfrozen layers' parameters\n",
    "#use a very small learning rate \n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p : p.requires_grad, model_classifier.parameters()),\n",
    "    lr = 1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5f14870",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bf84106",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62f4828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Train]:   0%|          | 0/1501 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Train]: 100%|██████████| 1501/1501 [05:05<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Train Loss: 0.6005, Acc: 0.7328, F1: 0.7151 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 [Train]: 100%|██████████| 1501/1501 [03:48<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Train Loss: 0.5904, Acc: 0.7346, F1: 0.7189 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 [Train]: 100%|██████████| 1501/1501 [03:41<00:00,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Train Loss: 0.5839, Acc: 0.7367, F1: 0.7213 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_classifier.train()\n",
    "    train_loss, train_labels, train_preds = 0.0, [], []\n",
    "\n",
    "    for images, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        #set the greadients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward pass\n",
    "        outputs = model_classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_preds.extend(outputs.argmax(1).detach().cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss/len(train_dataloader):.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "955e4402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Val]: 100%|██████████| 167/167 [00:41<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Val Loss: 0.8051, Acc: 0.8134, F1: 0.8138, AUC: nan |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 [Val]: 100%|██████████| 167/167 [00:29<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Val Loss: 1.0042, Acc: 0.6456, F1: 0.6248, AUC: nan |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 [Val]: 100%|██████████| 167/167 [00:29<00:00,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Val Loss: 0.7554, Acc: 0.6613, F1: 0.6384, AUC: nan |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model_classifier.eval()\n",
    "    val_loss, val_labels, val_preds = 0.0, [], []\n",
    "\n",
    "    for images, labels in tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        #set the greadients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward pass\n",
    "        outputs = model_classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "        val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc   = accuracy_score(val_labels, val_preds)\n",
    "    val_f1    = f1_score(val_labels, val_preds, average=\"macro\")\n",
    "\n",
    "    try:\n",
    "        val_auc = roc_auc_score(val_labels, val_preds, multi_class=\"ovr\")\n",
    "    except:\n",
    "        val_auc = float(\"nan\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Val Loss: {val_loss/len(val_dataloader):.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83618fa2",
   "metadata": {},
   "source": [
    "Ok, there is error here. Now, here is the fixed validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3be92f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 [Val]: 100%|██████████| 167/167 [00:28<00:00,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Val Loss: 1.3829, Acc: 0.5069, F1: 0.3709, AUC: 0.8211 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Corrected validation loop\n",
    "model_classifier.eval() # Set model to evaluation mode\n",
    "val_loss, val_labels, val_preds = 0.0, [], []\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation\n",
    "    for images, labels in tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # To fix the AUC issue, collect probability scores for each class\n",
    "        # Use torch.softmax() to convert logits to probabilities\n",
    "        probs = torch.softmax(outputs, dim=1) \n",
    "        val_preds.extend(probs.detach().cpu().numpy())\n",
    "        val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "# After the loop, you can calculate metrics\n",
    "val_acc = accuracy_score(val_labels, np.argmax(val_preds, axis=1))\n",
    "val_f1 = f1_score(val_labels, np.argmax(val_preds, axis=1), average=\"macro\")\n",
    "val_auc = roc_auc_score(val_labels, val_preds, multi_class=\"ovr\")\n",
    "\n",
    "print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "      f\"Val Loss: {val_loss/len(val_dataloader):.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35661c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_classifier.state_dict(), 'resnet_classifier_finetuned.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1fbe5d",
   "metadata": {},
   "source": [
    "I want to train this again. Changes: 1. Train for 5 epochs. 2. Unfreeze more layers. 3. Add more transformations to images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1afdde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache has been cleared.\n"
     ]
    }
   ],
   "source": [
    "# This function is crucial for freeing up GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU cache has been cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dc28b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define transformation for training set\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "                         \n",
    "])\n",
    "\n",
    "#define transformation for validation set\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "                         \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88797336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "##Loading the previously saved model\n",
    "\n",
    "# 1. Instantiate the model architecture\n",
    "num_classes = 3\n",
    "model_classifier = models.resnet50()\n",
    "num_ftrs = model_classifier.fc.in_features\n",
    "model_classifier.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# 2. Load the state dictionary from the saved file\n",
    "model_classifier.load_state_dict(torch.load('resnet_classifier_finetuned.pth'))\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_classifier.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_classifier.eval()\n",
    "\n",
    "print(\"Model has been loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "589823f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting all the layers to not require gradients\n",
    "#freezing layers\n",
    "for param in model_classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#unfreezing the final layer\n",
    "for param in model_classifier.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#unfreezing the layer 3 parameters\n",
    "for param in model_classifier.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#unfreezing the layer 4 parameters\n",
    "for param in model_classifier.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#define a new optimizer that includes the unfrozen layers' parameters\n",
    "#use a very small learning rate \n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p : p.requires_grad, model_classifier.parameters()),\n",
    "    lr = 1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79105a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df03b23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]:   0%|          | 0/1501 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 1501/1501 [09:26<00:00,  2.65it/s]\n",
      "Epoch 1/5 [Val]: 100%|██████████| 167/167 [00:38<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 0.5294, Acc: 0.7652, F1: 0.7532 | Val Loss: 0.6275, Acc: 0.7269, F1: 0.7107, AUC: 0.8730 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 1501/1501 [03:41<00:00,  6.77it/s]\n",
      "Epoch 2/5 [Val]: 100%|██████████| 167/167 [00:33<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Train Loss: 0.5196, Acc: 0.7702, F1: 0.7583 | Val Loss: 0.6445, Acc: 0.7186, F1: 0.7025, AUC: 0.8688 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 1501/1501 [03:46<00:00,  6.63it/s]\n",
      "Epoch 3/5 [Val]: 100%|██████████| 167/167 [00:33<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Train Loss: 0.5093, Acc: 0.7764, F1: 0.7659 | Val Loss: 0.6471, Acc: 0.7164, F1: 0.6964, AUC: 0.8700 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 1501/1501 [03:46<00:00,  6.63it/s]\n",
      "Epoch 4/5 [Val]: 100%|██████████| 167/167 [00:33<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Train Loss: 0.4953, Acc: 0.7858, F1: 0.7755 | Val Loss: 0.6597, Acc: 0.7194, F1: 0.7014, AUC: 0.8669 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|██████████| 1501/1501 [03:48<00:00,  6.58it/s]\n",
      "Epoch 5/5 [Val]: 100%|██████████| 167/167 [00:33<00:00,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Train Loss: 0.4814, Acc: 0.7926, F1: 0.7833 | Val Loss: 0.6735, Acc: 0.7224, F1: 0.7072, AUC: 0.8670 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ==================== Training Loop ====================\n",
    "    model_classifier.train()\n",
    "    train_loss, train_labels, train_preds = 0.0, [], []\n",
    "\n",
    "    for images, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Append predictions and labels for evaluation\n",
    "        train_preds.extend(outputs.argmax(1).detach().cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate training metrics\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "\n",
    "    # ==================== Validation Loop ====================\n",
    "    model_classifier.eval() # Set model to evaluation mode\n",
    "    val_loss, val_labels, val_preds_probs, val_preds_hard = 0.0, [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model_classifier(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Collect probability scores for AUC and hard predictions for accuracy\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            val_preds_probs.extend(probs.detach().cpu().numpy())\n",
    "            val_preds_hard.extend(outputs.argmax(1).detach().cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    val_acc = accuracy_score(val_labels, val_preds_hard)\n",
    "    val_f1 = f1_score(val_labels, val_preds_hard, average=\"macro\")\n",
    "    try:\n",
    "        val_auc = roc_auc_score(val_labels, val_preds_probs, multi_class=\"ovr\")\n",
    "    except:\n",
    "        val_auc = float(\"nan\")\n",
    "\n",
    "    # ==================== Print Metrics ====================\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss/len(train_dataloader):.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \"\n",
    "          f\"Val Loss: {val_loss/len(val_dataloader):.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8cc6b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_classifier.state_dict(), 'resnet_classifier_finetuned2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b76377",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "- Need to address overfitting\n",
    "- add weight decay to optimizer\n",
    "- introduce dropout\n",
    "- expand data agmentation\n",
    "- implement early stopping\n",
    "- adjust learning rate\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "self.classifier = nn.Sequential(\n",
    "    nn.Linear(num_ftrs + 64, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5), # Add this line\n",
    "    nn.Linear(256, num_classes)\n",
    ")\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e4b99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a computing device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f405a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache has been cleared.\n"
     ]
    }
   ],
   "source": [
    "# This function is crucial for freeing up GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU cache has been cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6df653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define transformation for training set\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness = 0.1, contrast = 0.1), #adding color jitter\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5), #adding random perspective\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "                         \n",
    "])\n",
    "\n",
    "#define transformation for validation set\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness = 0.1, contrast = 0.1), #adding color jitter\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5), #adding random perspective\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "                         \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "550d65c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "##Loading the previously saved model\n",
    "\n",
    "# 1. Instantiate the model architecture\n",
    "num_classes = 3\n",
    "model_classifier = models.resnet50()\n",
    "num_ftrs = model_classifier.fc.in_features\n",
    "model_classifier.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# 2. Load the state dictionary from the saved file\n",
    "saved_state_dict = torch.load('resnet_classifier_finetuned2.pth')\n",
    "model_classifier.load_state_dict(saved_state_dict)\n",
    "\n",
    "# Create the new Sequential model with the Dropout and Linear layers\n",
    "num_ftrs = 2048 # ResNet-50 features\n",
    "num_classes = 3\n",
    "new_fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(num_ftrs, num_classes)\n",
    ")\n",
    "\n",
    "# Extract the weights from the old state dictionary\n",
    "old_fc_weight = saved_state_dict['fc.weight']\n",
    "old_fc_bias = saved_state_dict['fc.bias']\n",
    "\n",
    "# Load the weights into the new Linear layer\n",
    "new_fc[1].weight = nn.Parameter(old_fc_weight)\n",
    "new_fc[1].bias = nn.Parameter(old_fc_bias)\n",
    "\n",
    "# Replace the model's fully connected layer with the new Sequential model\n",
    "model_classifier.fc = new_fc\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "model_classifier.to(device)\n",
    "\n",
    "print(\"Model has been loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a572895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting all the layers to not require gradients\n",
    "#freezing layers\n",
    "for param in model_classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#unfreezing the final layer\n",
    "for param in model_classifier.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#unfreezing the layer 3 parameters\n",
    "for param in model_classifier.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#unfreezing the layer 4 parameters\n",
    "for param in model_classifier.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#define a new optimizer that includes the unfrozen layers' parameters\n",
    "#use a very small learning rate \n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p : p.requires_grad, model_classifier.parameters()),\n",
    "    lr = 1e-5, #reducing learning rate to 1e-5 from 1e-4\n",
    "    weight_decay=1e-5 #adding weight decay of 1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f191ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root = train_dir, transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(root = val_dir, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3264d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6b10008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]:   0%|          | 0/1501 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 1501/1501 [07:38<00:00,  3.28it/s]\n",
      "Epoch 1/5 [Val]: 100%|██████████| 167/167 [00:46<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 0.6707, Acc: 0.7013, F1: 0.6895 | Val Loss: 0.6899, Acc: 0.6995, F1: 0.6854, AUC: 0.8540 |\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 1501/1501 [05:26<00:00,  4.59it/s]\n",
      "Epoch 2/5 [Val]: 100%|██████████| 167/167 [00:47<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Train Loss: 0.6475, Acc: 0.7083, F1: 0.6954 | Val Loss: 0.6670, Acc: 0.6939, F1: 0.6785, AUC: 0.8580 |\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 1501/1501 [04:54<00:00,  5.09it/s]\n",
      "Epoch 3/5 [Val]: 100%|██████████| 167/167 [00:47<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Train Loss: 0.6339, Acc: 0.7110, F1: 0.6979 | Val Loss: 0.6714, Acc: 0.6980, F1: 0.6815, AUC: 0.8549 |\n",
      "Validation loss has not improved for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 1501/1501 [04:40<00:00,  5.35it/s]\n",
      "Epoch 4/5 [Val]: 100%|██████████| 167/167 [00:43<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Train Loss: 0.6324, Acc: 0.7150, F1: 0.7009 | Val Loss: 0.6598, Acc: 0.7010, F1: 0.6844, AUC: 0.8602 |\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|██████████| 1501/1501 [04:49<00:00,  5.19it/s]\n",
      "Epoch 5/5 [Val]: 100%|██████████| 167/167 [00:39<00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Train Loss: 0.6271, Acc: 0.7189, F1: 0.7044 | Val Loss: 0.6513, Acc: 0.7100, F1: 0.6921, AUC: 0.8633 |\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 5\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ==================== Training Loop ====================\n",
    "    model_classifier.train()\n",
    "    train_loss, train_labels, train_preds = 0.0, [], []\n",
    "\n",
    "    for images, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Append predictions and labels for evaluation\n",
    "        train_preds.extend(outputs.argmax(1).detach().cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate training metrics\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "\n",
    "    # ==================== Validation Loop ====================\n",
    "    model_classifier.eval() # Set model to evaluation mode\n",
    "    val_loss, val_labels, val_preds_probs, val_preds_hard = 0.0, [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model_classifier(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Collect probability scores for AUC and hard predictions for accuracy\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            val_preds_probs.extend(probs.detach().cpu().numpy())\n",
    "            val_preds_hard.extend(outputs.argmax(1).detach().cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    val_acc = accuracy_score(val_labels, val_preds_hard)\n",
    "    val_f1 = f1_score(val_labels, val_preds_hard, average=\"macro\")\n",
    "    try:\n",
    "        val_auc = roc_auc_score(val_labels, val_preds_probs, multi_class=\"ovr\")\n",
    "    except:\n",
    "        val_auc = float(\"nan\")\n",
    "\n",
    "    # ==================== Print Metrics ====================\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss/len(train_dataloader):.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \"\n",
    "          f\"Val Loss: {val_loss/len(val_dataloader):.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f} |\")\n",
    "    \n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        print(\"Validation loss improved. Saving model.\")\n",
    "        #save the best model\n",
    "        torch.save(model_classifier.state_dict(), 'best_resnet_classifier3.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Validation loss has not improved for {patience_counter} epochs.\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13858e45",
   "metadata": {},
   "source": [
    "more training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "690feb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 1501/1501 [04:19<00:00,  5.77it/s]\n",
      "Epoch 1/10 [Val]: 100%|██████████| 167/167 [00:38<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6232, Acc: 0.7185, F1: 0.7034 | Val Loss: 0.6631, Acc: 0.7033, F1: 0.6850, AUC: 0.8601 |\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 1501/1501 [04:36<00:00,  5.44it/s]\n",
      "Epoch 2/10 [Val]: 100%|██████████| 167/167 [00:37<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.6209, Acc: 0.7235, F1: 0.7091 | Val Loss: 0.6542, Acc: 0.7141, F1: 0.6990, AUC: 0.8622 |\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 1501/1501 [04:25<00:00,  5.65it/s]\n",
      "Epoch 3/10 [Val]: 100%|██████████| 167/167 [00:37<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.6225, Acc: 0.7198, F1: 0.7058 | Val Loss: 0.6494, Acc: 0.7051, F1: 0.6898, AUC: 0.8641 |\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 1501/1501 [04:26<00:00,  5.64it/s]\n",
      "Epoch 4/10 [Val]: 100%|██████████| 167/167 [00:37<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.6198, Acc: 0.7213, F1: 0.7067 | Val Loss: 0.6460, Acc: 0.7138, F1: 0.6960, AUC: 0.8646 |\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 1501/1501 [04:29<00:00,  5.56it/s]\n",
      "Epoch 5/10 [Val]: 100%|██████████| 167/167 [00:37<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.6175, Acc: 0.7209, F1: 0.7059 | Val Loss: 0.6447, Acc: 0.7141, F1: 0.6965, AUC: 0.8663 |\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 1501/1501 [04:31<00:00,  5.54it/s]\n",
      "Epoch 6/10 [Val]: 100%|██████████| 167/167 [00:37<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.6177, Acc: 0.7220, F1: 0.7073 | Val Loss: 0.6441, Acc: 0.7108, F1: 0.6889, AUC: 0.8660 |\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 1501/1501 [04:25<00:00,  5.66it/s]\n",
      "Epoch 7/10 [Val]: 100%|██████████| 167/167 [00:37<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.6121, Acc: 0.7275, F1: 0.7133 | Val Loss: 0.6403, Acc: 0.7104, F1: 0.6933, AUC: 0.8677 |\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 1501/1501 [04:32<00:00,  5.52it/s]\n",
      "Epoch 8/10 [Val]: 100%|██████████| 167/167 [00:37<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.6140, Acc: 0.7222, F1: 0.7075 | Val Loss: 0.6350, Acc: 0.7063, F1: 0.6917, AUC: 0.8685 |\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 1501/1501 [04:24<00:00,  5.68it/s]\n",
      "Epoch 9/10 [Val]: 100%|██████████| 167/167 [00:37<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.6149, Acc: 0.7245, F1: 0.7102 | Val Loss: 0.6424, Acc: 0.7152, F1: 0.6994, AUC: 0.8663 |\n",
      "Validation loss has not improved for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 1501/1501 [04:27<00:00,  5.60it/s]\n",
      "Epoch 10/10 [Val]: 100%|██████████| 167/167 [00:40<00:00,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.6097, Acc: 0.7247, F1: 0.7103 | Val Loss: 0.6481, Acc: 0.7085, F1: 0.6919, AUC: 0.8638 |\n",
      "Validation loss has not improved for 2 epochs.\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 10\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ==================== Training Loop ====================\n",
    "    model_classifier.train()\n",
    "    train_loss, train_labels, train_preds = 0.0, [], []\n",
    "\n",
    "    for images, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Append predictions and labels for evaluation\n",
    "        train_preds.extend(outputs.argmax(1).detach().cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate training metrics\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "\n",
    "    # ==================== Validation Loop ====================\n",
    "    model_classifier.eval() # Set model to evaluation mode\n",
    "    val_loss, val_labels, val_preds_probs, val_preds_hard = 0.0, [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model_classifier(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Collect probability scores for AUC and hard predictions for accuracy\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            val_preds_probs.extend(probs.detach().cpu().numpy())\n",
    "            val_preds_hard.extend(outputs.argmax(1).detach().cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    val_acc = accuracy_score(val_labels, val_preds_hard)\n",
    "    val_f1 = f1_score(val_labels, val_preds_hard, average=\"macro\")\n",
    "    try:\n",
    "        val_auc = roc_auc_score(val_labels, val_preds_probs, multi_class=\"ovr\")\n",
    "    except:\n",
    "        val_auc = float(\"nan\")\n",
    "\n",
    "    # ==================== Print Metrics ====================\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss/len(train_dataloader):.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \"\n",
    "          f\"Val Loss: {val_loss/len(val_dataloader):.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f} |\")\n",
    "    \n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        print(\"Validation loss improved. Saving model.\")\n",
    "        #save the best model\n",
    "        torch.save(model_classifier.state_dict(), 'best_resnet_classifier3_more_trained.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Validation loss has not improved for {patience_counter} epochs.\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0956307",
   "metadata": {},
   "source": [
    "Ok. This is good enough accuract for a stand alone model. Now I want to include the meta data along with the images for training. Hopefully, this will increase classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "336d9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a computing device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3236a00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75884c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache has been cleared.\n"
     ]
    }
   ],
   "source": [
    "# This function is crucial for freeing up GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU cache has been cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c13e3b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aalap\\AppData\\Local\\Temp\\ipykernel_29740\\3273134818.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_classifier.load_state_dict(torch.load('best_resnet_classifier3_more_trained.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been loaded successfully on cuda.\n"
     ]
    }
   ],
   "source": [
    "#load the latest model\n",
    "num_classes = 3\n",
    "num_ftrs = 2048 \n",
    "\n",
    "model_classifier = models.resnet50()\n",
    "model_classifier.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(num_ftrs, num_classes)\n",
    ")\n",
    "\n",
    "model_classifier.load_state_dict(torch.load('best_resnet_classifier3_more_trained.pth'))\n",
    "\n",
    "model_classifier.to(device)\n",
    "model_classifier.eval()\n",
    "\n",
    "print(f\"Model has been loaded successfully on {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b9192",
   "metadata": {},
   "source": [
    "I realized that validation transformations are not supposed to have augmentation. This results in model unnecessarily testing on harder images and thus showing lower validation score.\n",
    "Will try to fix this for now by running a standalone validation pass using clean val_transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18d69d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:39<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your TRUE Validation Accuracy is: 0.7186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#train_dataset = datasets.ImageFolder(root = train_dir, transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(root = val_dir, transform=clean_val_transforms)\n",
    "\n",
    "#BATCH_SIZE = 16\n",
    "\n",
    "# train_dataloader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size = BATCH_SIZE,\n",
    "#     shuffle = True,\n",
    "#     num_workers = 2,\n",
    "#     pin_memory = True\n",
    "# )\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "\n",
    "model_classifier.eval()\n",
    "val_labels, val_preds_probs = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_classifier(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        val_preds_probs.extend(probs.cpu().numpy())\n",
    "        val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "val_preds_hard = np.argmax(val_preds_probs, axis=1)\n",
    "true_acc = accuracy_score(val_labels, val_preds_hard)\n",
    "print(f\"Your TRUE Validation Accuracy is: {true_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c21cb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_classifier.state_dict(), 'final_resnet_before_adding_metadata.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca775fc0",
   "metadata": {},
   "source": [
    "Now, working on the metadata to help model intake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42a137fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientId</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>Target</th>\n",
       "      <th>class</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>modality</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>No Lung Opacity / Not Normal</td>\n",
       "      <td>51</td>\n",
       "      <td>F</td>\n",
       "      <td>CR</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>No Lung Opacity / Not Normal</td>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>CR</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00322d4d-1c29-4943-afc9-b6754be640eb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>No Lung Opacity / Not Normal</td>\n",
       "      <td>19</td>\n",
       "      <td>M</td>\n",
       "      <td>CR</td>\n",
       "      <td>AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003d8fa0-6bf1-40ed-b54c-ac657f8495c5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>28</td>\n",
       "      <td>M</td>\n",
       "      <td>CR</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n",
       "      <td>264.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Lung Opacity</td>\n",
       "      <td>32</td>\n",
       "      <td>F</td>\n",
       "      <td>CR</td>\n",
       "      <td>AP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              patientId      x      y  width  height  Target  \\\n",
       "0  0004cfab-14fd-4e49-80ba-63a80b6bddd6    NaN    NaN    NaN     NaN       0   \n",
       "1  00313ee0-9eaa-42f4-b0ab-c148ed3241cd    NaN    NaN    NaN     NaN       0   \n",
       "2  00322d4d-1c29-4943-afc9-b6754be640eb    NaN    NaN    NaN     NaN       0   \n",
       "3  003d8fa0-6bf1-40ed-b54c-ac657f8495c5    NaN    NaN    NaN     NaN       0   \n",
       "4  00436515-870c-4b36-a041-de91049b9ab4  264.0  152.0  213.0   379.0       1   \n",
       "\n",
       "                          class  age sex modality position  \n",
       "0  No Lung Opacity / Not Normal   51   F       CR       PA  \n",
       "1  No Lung Opacity / Not Normal   48   F       CR       PA  \n",
       "2  No Lung Opacity / Not Normal   19   M       CR       AP  \n",
       "3                        Normal   28   M       CR       PA  \n",
       "4                  Lung Opacity   32   F       CR       AP  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('stage2_train_metadata.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "735753e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing categorical variables of interest to numerical values\n",
    "\n",
    "#Sex: Male = 0, Female = 1\n",
    "df['sex'] = df['sex'].map({'M': 0, 'F': 1})\n",
    "\n",
    "#position: AP = 0, PA = 1\n",
    "df['position'] = df['position'].map({'AP': 0, 'PA': 1})\n",
    "\n",
    "#scale age feature to be from 0 to 1\n",
    "scaler = MinMaxScaler()\n",
    "df['age'] = scaler.fit_transform(df[['age']])\n",
    "\n",
    "#we are not including modality as it has only one unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb7f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Map Folder Structure to Paths ---\n",
    "import os\n",
    "\n",
    "def get_path_dict(directory):\n",
    "    \"\"\"Creates a dictionary mapping patientId to the absolute path of their image file.\"\"\"\n",
    "    path_dict = {}\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.png', '.jpg')):\n",
    "                pid = os.path.splitext(file)[0]\n",
    "                path_dict[pid] = os.path.join(root, file)\n",
    "    return path_dict\n",
    "\n",
    "train_path_dict = get_path_dict(train_dir) \n",
    "val_path_dict = get_path_dict(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094a1b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('000db696-cf54-4385-b10b-6b16fbb3f985',\n",
       " 'C:\\\\Users\\\\Aalap\\\\Documents\\\\ML\\\\rnsa\\\\training\\\\lung_opacity\\\\000db696-cf54-4385-b10b-6b16fbb3f985.png')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = list(train_path_dict.items())\n",
    "temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fb05c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Synchronize CSV Labels (3 Classes) ---\n",
    "class_mapping = {'Normal': 0,\n",
    "                 'No Lung Opacity / Not Normal' : 1,\n",
    "                 'Lung Opacity' : 2}\n",
    "\n",
    "# Ensure the 'class' column strings match folder names\n",
    "df['Target_3Class'] = df['class'].map(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feab957d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientId</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>Target</th>\n",
       "      <th>class</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>modality</th>\n",
       "      <th>position</th>\n",
       "      <th>Target_3Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>No Lung Opacity / Not Normal</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>1</td>\n",
       "      <td>CR</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>No Lung Opacity / Not Normal</td>\n",
       "      <td>0.305195</td>\n",
       "      <td>1</td>\n",
       "      <td>CR</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00322d4d-1c29-4943-afc9-b6754be640eb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>No Lung Opacity / Not Normal</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0</td>\n",
       "      <td>CR</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003d8fa0-6bf1-40ed-b54c-ac657f8495c5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.175325</td>\n",
       "      <td>0</td>\n",
       "      <td>CR</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n",
       "      <td>264.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Lung Opacity</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>1</td>\n",
       "      <td>CR</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              patientId      x      y  width  height  Target  \\\n",
       "0  0004cfab-14fd-4e49-80ba-63a80b6bddd6    NaN    NaN    NaN     NaN       0   \n",
       "1  00313ee0-9eaa-42f4-b0ab-c148ed3241cd    NaN    NaN    NaN     NaN       0   \n",
       "2  00322d4d-1c29-4943-afc9-b6754be640eb    NaN    NaN    NaN     NaN       0   \n",
       "3  003d8fa0-6bf1-40ed-b54c-ac657f8495c5    NaN    NaN    NaN     NaN       0   \n",
       "4  00436515-870c-4b36-a041-de91049b9ab4  264.0  152.0  213.0   379.0       1   \n",
       "\n",
       "                          class       age  sex modality  position  \\\n",
       "0  No Lung Opacity / Not Normal  0.324675    1       CR         1   \n",
       "1  No Lung Opacity / Not Normal  0.305195    1       CR         1   \n",
       "2  No Lung Opacity / Not Normal  0.116883    0       CR         0   \n",
       "3                        Normal  0.175325    0       CR         1   \n",
       "4                  Lung Opacity  0.201299    1       CR         0   \n",
       "\n",
       "   Target_3Class  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              0  \n",
       "4              2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "481eeba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows mapped successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check if any rows failed to map (will result in NaN)\n",
    "nans = df['Target_3Class'].isna().sum()\n",
    "if nans > 0:\n",
    "    print(f\"Warning: {nans} rows failed to map. Check your class names!\")\n",
    "    print(\"Unique values in CSV:\", df['class'].unique())\n",
    "else:\n",
    "    print(\"All rows mapped successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3696c260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:  (24181, 12)\n",
      "Validation set shape:  (6046, 12)\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df, test_size = 0.2, stratify=df['Target_3Class'], random_state=42)\n",
    "\n",
    "print(\"Training set shape: \", train_df.shape)\n",
    "print(\"Validation set shape: \", val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68270030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, metadata_features, fine_tuned_model):\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "\n",
    "        #Image branch: use the existing fine-tuned model\n",
    "        self.image_model = fine_tuned_model\n",
    "\n",
    "        if isinstance(self.image_model.fc, nn.Sequential):\n",
    "            num_ftrs = self.image_model.fc[1].in_features\n",
    "        else:\n",
    "            num_ftrs = self.image_model.fc.in_features\n",
    "\n",
    "        self.image_model.fc = nn.Identity() #remove the final layer\n",
    "\n",
    "        #Metadata branch\n",
    "        self.metadata_fc = nn.Sequential(\n",
    "            nn.Linear(metadata_features, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16)#,\n",
    "            #nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs + 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, metadata):\n",
    "            img_features = self.image_model(image)\n",
    "            meta_features = self.metadata_fc(metadata)\n",
    "            combined = torch.cat((img_features, meta_features), dim=1)\n",
    "            logits = self.classifier(combined)\n",
    "            return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed49bf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-modal model created successfully.\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3\n",
    "num_ftrs = 2048\n",
    "\n",
    "model_classifier = models.resnet50()\n",
    "model_classifier.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_ftrs, num_classes)\n",
    ")\n",
    "\n",
    "#load the weights\n",
    "model_classifier.load_state_dict(torch.load('final_resnet_before_adding_metadata.pth', weights_only=True))\n",
    "model_classifier.to(device)\n",
    "\n",
    "#wrap this into MultiModalModel class\n",
    "final_model = MultiModalClassifier(\n",
    "    num_classes = num_classes,\n",
    "    metadata_features = 3,\n",
    "    fine_tuned_model = model_classifier\n",
    ")\n",
    "\n",
    "final_model.to(device)\n",
    "\n",
    "print(\"Multi-modal model created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c90cbf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Is model on CUDA? True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")\n",
    "print(f\"Is model on CUDA? {next(final_model.parameters()).is_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d6aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([\n",
    "    {'params': final_model.image_model.parameters(), 'lr': 1e-6},\n",
    "    {'params': final_model.metadata_fc.parameters(), 'lr': 1e-4},\n",
    "    {'params': final_model.classifier.parameters(), 'lr': 1e-4}\n",
    "], weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0498d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class RSNA_MultimodalDataset(Dataset):\n",
    "    def __init__(self, metadata_df, path_dict, transform = None):\n",
    "        self.df = metadata_df[metadata_df['patientId'].isin(path_dict.keys())].reset_index(drop=True)\n",
    "        self.path_dict = path_dict\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        pid = row['patientId']\n",
    "        \n",
    "        img_path = self.path_dict.get(pid)\n",
    "        \n",
    "        # DEBUGGING: If path is missing, print it!\n",
    "        if img_path is None:\n",
    "            print(f\"MISSING IMAGE: {pid}\") \n",
    "            # If you see this scrolling endlessly, your mapping is broken.\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Metadata: [age, sex, position]\n",
    "        metadata = torch.tensor([row['age'], row['sex'], row['position']], dtype=torch.float32)\n",
    "        label = int(row['Target_3Class'])\n",
    "\n",
    "        return image, metadata, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e159af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training transformation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#validation transformation\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44ab32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate datasets\n",
    "train_ds = RSNA_MultimodalDataset(metadata_df = train_df, path_dict = train_path_dict, transform=train_transforms)\n",
    "val_ds = RSNA_MultimodalDataset(metadata_df = val_df, path_dict = val_path_dict, transform=val_transforms)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "#Create DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    num_workers = 0,\n",
    "    pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f736375d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: {'Normal': 0, 'No Lung Opacity / Not Normal': 1, 'Lung Opacity': 2}\n",
      "Calculated weights: tensor([1.1383, 0.8524, 1.0545], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(train_df['Target_3Class'])\n",
    "weights = compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=classes, \n",
    "    y=train_df['Target_3Class']\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(f\"Class mapping: {class_mapping}\")\n",
    "print(f\"Calculated weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12bec14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|██████████| 1358/1358 [13:37<00:00,  1.66it/s]\n",
      "Epoch 1/15 [Val]: 100%|██████████| 37/37 [00:19<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.6469, Acc: 0.7050, F1: 0.7082 | Val Loss: 0.6205, Acc: 0.7254, F1: 0.7256, AUC: 0.8857 |\n",
      "\n",
      "Class-wise Validation Accuracies:\n",
      "  Normal: 0.9176\n",
      "  No Lung Opacity / Not Normal: 0.5689\n",
      "  Lung Opacity: 0.7267\n",
      "  -> Normal: 0.9176\n",
      "  -> No Lung Opacity / Not Normal: 0.5689\n",
      "  -> Lung Opacity: 0.7267\n",
      "Validation loss improved. Saving MultiModal Model...\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Train]: 100%|██████████| 1358/1358 [10:55<00:00,  2.07it/s]\n",
      "Epoch 2/15 [Val]: 100%|██████████| 37/37 [00:13<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 | Train Loss: 0.6173, Acc: 0.7193, F1: 0.7235 | Val Loss: 0.5923, Acc: 0.7375, F1: 0.7387, AUC: 0.8883 |\n",
      "\n",
      "Class-wise Validation Accuracies:\n",
      "  Normal: 0.9066\n",
      "  No Lung Opacity / Not Normal: 0.5867\n",
      "  Lung Opacity: 0.7558\n",
      "  -> Normal: 0.9066\n",
      "  -> No Lung Opacity / Not Normal: 0.5867\n",
      "  -> Lung Opacity: 0.7558\n",
      "Validation loss improved. Saving MultiModal Model...\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Train]: 100%|██████████| 1358/1358 [10:52<00:00,  2.08it/s]\n",
      "Epoch 3/15 [Val]: 100%|██████████| 37/37 [00:13<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 | Train Loss: 0.6085, Acc: 0.7172, F1: 0.7216 | Val Loss: 0.5972, Acc: 0.7202, F1: 0.7190, AUC: 0.8878 |\n",
      "\n",
      "Class-wise Validation Accuracies:\n",
      "  Normal: 0.9231\n",
      "  No Lung Opacity / Not Normal: 0.5200\n",
      "  Lung Opacity: 0.7674\n",
      "  -> Normal: 0.9231\n",
      "  -> No Lung Opacity / Not Normal: 0.5200\n",
      "  -> Lung Opacity: 0.7674\n",
      "No improvement for 1 epochs.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Train]: 100%|██████████| 1358/1358 [10:49<00:00,  2.09it/s]\n",
      "Epoch 4/15 [Val]: 100%|██████████| 37/37 [00:13<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 | Train Loss: 0.6027, Acc: 0.7231, F1: 0.7277 | Val Loss: 0.5948, Acc: 0.7340, F1: 0.7364, AUC: 0.8867 |\n",
      "\n",
      "Class-wise Validation Accuracies:\n",
      "  Normal: 0.8901\n",
      "  No Lung Opacity / Not Normal: 0.6089\n",
      "  Lung Opacity: 0.7326\n",
      "  -> Normal: 0.8901\n",
      "  -> No Lung Opacity / Not Normal: 0.6089\n",
      "  -> Lung Opacity: 0.7326\n",
      "No improvement for 2 epochs.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Train]: 100%|██████████| 1358/1358 [10:46<00:00,  2.10it/s]\n",
      "Epoch 5/15 [Val]: 100%|██████████| 37/37 [00:13<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 | Train Loss: 0.5981, Acc: 0.7232, F1: 0.7281 | Val Loss: 0.5941, Acc: 0.7340, F1: 0.7327, AUC: 0.8879 |\n",
      "\n",
      "Class-wise Validation Accuracies:\n",
      "  Normal: 0.9341\n",
      "  No Lung Opacity / Not Normal: 0.5422\n",
      "  Lung Opacity: 0.7733\n",
      "  -> Normal: 0.9341\n",
      "  -> No Lung Opacity / Not Normal: 0.5422\n",
      "  -> Lung Opacity: 0.7733\n",
      "No improvement for 3 epochs.\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#train and val loop\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "num_epochs = 15\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #===================== Training Loop ====================\n",
    "    final_model.train()\n",
    "    train_loss, train_labels, train_preds = 0.0, [], []\n",
    "\n",
    "    for images, metadata, labels in tqdm(train_dataloader, desc = f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        images = images.to(device)\n",
    "        metadata = metadata.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Forward pass with two outputs\n",
    "        outputs = final_model(images, metadata)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_preds.extend(outputs.argmax(1).detach().cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "\n",
    "    #===================== Validation Loop ====================\n",
    "\n",
    "    final_model.eval()\n",
    "    val_loss, val_labels, val_preds_probs, val_preds_hard = 0.0, [], [], []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, metadata, labels in tqdm(val_dataloader, desc = f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "            images = images.to(device)\n",
    "            metadata = metadata.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = final_model(images, metadata)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(outputs, dim = 1)\n",
    "            val_preds_probs.extend(probs.detach().cpu().numpy())\n",
    "            val_preds_hard.extend(outputs.argmax(1).detach().cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    \n",
    "    # --- Calculate Metrics ---\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_acc = accuracy_score(val_labels, val_preds_hard)\n",
    "    val_f1 = f1_score(val_labels, val_preds_hard, average=\"macro\")\n",
    "\n",
    "    try:\n",
    "        val_auc = roc_auc_score(val_labels, val_preds_probs, multi_class=\"ovr\")\n",
    "    except:\n",
    "        val_auc = float(\"nan\")\n",
    "\n",
    "    # --- Print Metrics ---\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss/len(train_dataloader):.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \"\n",
    "          f\"Val Loss: {val_loss/len(val_dataloader):.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f} |\")\n",
    "    \n",
    "    \n",
    "    #check accuracy per class\n",
    "    cm = confusion_matrix(val_labels, val_preds_hard, labels = [0,1,2])\n",
    "    class_accuracies = cm.diagonal() / (cm.sum(axis=1) + 1e-6)\n",
    "\n",
    "    print(\"\\nClass-wise Validation Accuracies:\")\n",
    "    for i, class_name in enumerate(class_mapping.keys()):\n",
    "        print(f\"  {class_name}: {class_accuracies[i]:.4f}\")\n",
    "\n",
    "    names = list(class_mapping.keys())\n",
    "    for i, class_name in enumerate(names):\n",
    "        print(f\"  -> {class_name}: {class_accuracies[i]:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        print(\"Validation loss improved. Saving MultiModal Model...\")\n",
    "        torch.save(final_model.state_dict(), 'best_multimodal_classifier.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement for {patience_counter} epochs.\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    print(\"-\" * 30)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "409a942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model.state_dict(), 'multimodal_classifier_iteration1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1666b0",
   "metadata": {},
   "source": [
    "Saliency Map Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3d5c8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-modal model loaded successfully for evaluation.\n"
     ]
    }
   ],
   "source": [
    "num_ftrs = 2048\n",
    "model_classifier = models.resnet50()\n",
    "model_classifier.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_ftrs, 3)\n",
    ")\n",
    "\n",
    "#recreate multimodel shell\n",
    "final_model = MultiModalClassifier(\n",
    "    num_classes=3,\n",
    "    metadata_features=3,\n",
    "    fine_tuned_model=model_classifier\n",
    ")\n",
    "\n",
    "#load the weights\n",
    "checkpoint_path = 'multimodal_classifier_iteration1.pth'\n",
    "state_dict = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "final_model.load_state_dict(state_dict)\n",
    "\n",
    "#move model to device\n",
    "final_model.to(device)\n",
    "\n",
    "#set model to evaluation mode\n",
    "final_model.eval()\n",
    "\n",
    "print(\"Multi-modal model loaded successfully for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3233becf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing Patient ID: f99c02a3-1731-4d3b-80c2-fc0cf713787e\n",
      "Saliency map saved to: saliency_test.png\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "def save_saliency_map(model, dataset, idx, device, save_path=\"saliency_check.png\"):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Get the image and metadata\n",
    "    img_tensor, meta, label = dataset[idx]\n",
    "    img_input = img_tensor.unsqueeze(0).to(device) # [1, 3, 224, 224]\n",
    "    meta_input = meta.unsqueeze(0).to(device).float()\n",
    "    \n",
    "    # 2. Hook into the last convolutional layer\n",
    "    # For ResNet50, this is layer4[-1]\n",
    "    target_layer = model.image_model.layer4[-1]\n",
    "    \n",
    "    feature_maps = []\n",
    "    gradients = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        feature_maps.append(output)\n",
    "        \n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        gradients.append(grad_out[0])\n",
    "        \n",
    "    # Register hooks\n",
    "    handle1 = target_layer.register_forward_hook(hook_fn)\n",
    "    handle2 = target_layer.register_full_backward_hook(backward_hook)\n",
    "    \n",
    "    # 3. Forward Pass\n",
    "    output = model(img_input, meta_input)\n",
    "    \n",
    "    # 4. Backward Pass (Focus on the 'Lung Opacity' class, which is index 2)\n",
    "    model.zero_grad()\n",
    "    score = output[0, 2] # Force it to explain why it thinks it's Class 2\n",
    "    score.backward()\n",
    "    \n",
    "    # 5. Process Gradients (The Math part)\n",
    "    grads = gradients[0] # [1, 2048, 7, 7]\n",
    "    fmap = feature_maps[0] # [1, 2048, 7, 7]\n",
    "    \n",
    "    # Global Average Pooling of gradients\n",
    "    weights = torch.mean(grads, dim=(2, 3), keepdim=True) # [1, 2048, 1, 1]\n",
    "    \n",
    "    # Weighted combination of feature maps\n",
    "    cam = torch.sum(weights * fmap, dim=1, keepdim=True) # [1, 1, 7, 7]\n",
    "    \n",
    "    # ReLU (Remove negative correlations)\n",
    "    cam = F.relu(cam)\n",
    "    \n",
    "    # Resize up to image size (224x224) using PyTorch Interpolation\n",
    "    cam = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    # Normalize 0-1\n",
    "    cam = cam - cam.min()\n",
    "    cam = cam / (cam.max() + 1e-7)\n",
    "    \n",
    "    # 6. Create the Overlay using PIL\n",
    "    # Convert tensor to numpy for PIL\n",
    "    heatmap_data = cam.squeeze().cpu().detach().numpy() # [224, 224]\n",
    "    heatmap_uint8 = (heatmap_data * 255).astype(np.uint8)\n",
    "    \n",
    "    # Create a \"Red\" heatmap image\n",
    "    # We make a red image where the transparency (Alpha) is controlled by the heatmap intensity\n",
    "    red_overlay = Image.new(\"RGB\", (224, 224), color=(255, 0, 0))\n",
    "    mask = Image.fromarray(heatmap_uint8)\n",
    "    \n",
    "    # Get original image\n",
    "    # Undo normalization to display it correctly\n",
    "    inv_normalize = torch.as_tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    mean = torch.as_tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    orig_img_tensor = img_tensor.cpu() * inv_normalize + mean\n",
    "    orig_img_tensor = torch.clamp(orig_img_tensor, 0, 1)\n",
    "    \n",
    "    # Convert original tensor to PIL\n",
    "    orig_pil = transforms.ToPILImage()(orig_img_tensor)\n",
    "    \n",
    "    # Composite the images\n",
    "    # We blend the Red Overlay onto the Original using the heatmap as the mask\n",
    "    final_image = Image.composite(red_overlay, orig_pil, mask)\n",
    "    \n",
    "    # Blend it slightly so you can see the bones *under* the red\n",
    "    final_output = Image.blend(orig_pil, final_image, alpha=0.5)\n",
    "    \n",
    "    # 7. Save to disk\n",
    "    final_output.save(save_path)\n",
    "    print(f\"Saliency map saved to: {save_path}\")\n",
    "    \n",
    "    # Clean up hooks\n",
    "    handle1.remove()\n",
    "    handle2.remove()\n",
    "\n",
    "# --- Run it on a 'Lung Opacity' case ---\n",
    "# Find a sample where the label is 2 (Lung Opacity)\n",
    "opacity_indices = [i for i, x in enumerate(val_ds.df['Target_3Class']) if x == 2]\n",
    "\n",
    "if len(opacity_indices) > 0:\n",
    "    idx_to_check = opacity_indices[0] # Pick the first one\n",
    "    print(f\"Visualizing Patient ID: {val_ds.df.iloc[idx_to_check]['patientId']}\")\n",
    "    save_saliency_map(final_model, val_ds, idx_to_check, device, \"saliency_test.png\")\n",
    "else:\n",
    "    print(\"No Opacity cases found in validation set to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f503e",
   "metadata": {},
   "source": [
    "Looking at saliency_test.png, the red \"heat\" is concentrated directly on the lung fields.\n",
    "\n",
    "- Left Side (Patient's Right Lung): There is a clear hotspot on the mid-to-lower lung zone.\n",
    "\n",
    "- Right Side (Patient's Left Lung): There is a massive area of activation covering almost the entire lung field.\n",
    "\n",
    "- What it ignored: Crucially, the model is ignoring the \"L\" tag in the top right corner, the shoulder joints, and the empty black space above the shoulders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73cd003",
   "metadata": {},
   "source": [
    "This confirms the model (77% accuracy on Opacity) is not \"cheating.\" It has learned that texture inside the ribcage is what matters for determining pneumonia. The diffuse red glow suggests it's picking up on the general \"haziness\" or \"consolidation\" characteristic of lung opacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb27f4a",
   "metadata": {},
   "source": [
    "The backbone and classifier are ready now. Can move ahead and build the Object Detection part using Faster R-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3426990b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6012 images with bounding boxes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientId</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000db696-cf54-4385-b10b-6b16fbb3f985</td>\n",
       "      <td>[316.0, 660.0]</td>\n",
       "      <td>[318.0, 375.0]</td>\n",
       "      <td>[170.0, 146.0]</td>\n",
       "      <td>[478.0, 402.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe35a-2649-43d4-b027-e67796d412e0</td>\n",
       "      <td>[570.0, 83.0]</td>\n",
       "      <td>[282.0, 227.0]</td>\n",
       "      <td>[269.0, 296.0]</td>\n",
       "      <td>[409.0, 438.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001031d9-f904-4a23-b3e5-2c088acd19c6</td>\n",
       "      <td>[66.0, 552.0]</td>\n",
       "      <td>[160.0, 164.0]</td>\n",
       "      <td>[373.0, 376.0]</td>\n",
       "      <td>[608.0, 676.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001916b8-3d30-4935-a5d1-8eaddb1646cd</td>\n",
       "      <td>[198.0]</td>\n",
       "      <td>[375.0]</td>\n",
       "      <td>[114.0]</td>\n",
       "      <td>[206.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0022073f-cec8-42ec-ab5f-bc2314649235</td>\n",
       "      <td>[575.0, 161.0]</td>\n",
       "      <td>[232.0, 230.0]</td>\n",
       "      <td>[246.0, 223.0]</td>\n",
       "      <td>[528.0, 486.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              patientId               x               y  \\\n",
       "0  000db696-cf54-4385-b10b-6b16fbb3f985  [316.0, 660.0]  [318.0, 375.0]   \n",
       "1  000fe35a-2649-43d4-b027-e67796d412e0   [570.0, 83.0]  [282.0, 227.0]   \n",
       "2  001031d9-f904-4a23-b3e5-2c088acd19c6   [66.0, 552.0]  [160.0, 164.0]   \n",
       "3  001916b8-3d30-4935-a5d1-8eaddb1646cd         [198.0]         [375.0]   \n",
       "4  0022073f-cec8-42ec-ab5f-bc2314649235  [575.0, 161.0]  [232.0, 230.0]   \n",
       "\n",
       "            width          height  \n",
       "0  [170.0, 146.0]  [478.0, 402.0]  \n",
       "1  [269.0, 296.0]  [409.0, 438.0]  \n",
       "2  [373.0, 376.0]  [608.0, 676.0]  \n",
       "3         [114.0]         [206.0]  \n",
       "4  [246.0, 223.0]  [528.0, 486.0]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#processing metadata file to group boxes\n",
    "import ast\n",
    "\n",
    "#load the metadata csv\n",
    "box_df = pd.read_csv('stage2_train_metadata.csv')\n",
    "\n",
    "#filer only for the lung opacity classes\n",
    "box_df = box_df[box_df['Target'] == 1]\n",
    "\n",
    "#group by patientId\n",
    "grouped_df = box_df.groupby('patientId').agg({\n",
    "    'x': list,\n",
    "    'y': list,\n",
    "    'width': list,\n",
    "    'height': list\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"Found {len(grouped_df)} images with bounding boxes.\")\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fcc9c8",
   "metadata": {},
   "source": [
    "Detection Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a84880ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNADetectionDataset(Dataset):\n",
    "    def __init__(self, dataframe, path_dict, transform=None, resize_to=(512, 512)):\n",
    "        self.df = dataframe[dataframe['patientId'].isin(path_dict.keys())].reset_index(drop=True)\n",
    "        self.path_dict = path_dict\n",
    "        self.transform = transform\n",
    "        self.resize_to = resize_to\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Load Image\n",
    "        row = self.df.iloc[idx]\n",
    "        pid = row['patientId']\n",
    "\n",
    "        # Use path_dict for instant lookup\n",
    "        image_path = self.path_dict[pid] \n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        w_original, h_original = img.size\n",
    "\n",
    "        # 2. Resize Image\n",
    "        img = img.resize(self.resize_to)\n",
    "\n",
    "        # 3. Convert Image to Tensor\n",
    "        # This converts [0, 255] -> [0.0, 1.0] automatically\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "\n",
    "        # 4. Handle Bounding Boxes\n",
    "        boxes = []\n",
    "\n",
    "        # Get lists of coordinates\n",
    "        xs = row['x']\n",
    "        ys = row['y']\n",
    "        ws = row['width']\n",
    "        hs = row['height']\n",
    "\n",
    "        # Calculate scaling factors\n",
    "        scale_x = self.resize_to[0] / w_original\n",
    "        scale_y = self.resize_to[1] / h_original\n",
    "\n",
    "        for i in range(len(xs)):\n",
    "            # Convert xywh to xyxy format\n",
    "            x_min = xs[i] * scale_x\n",
    "            y_min = ys[i] * scale_y\n",
    "            x_max = (xs[i] + ws[i]) * scale_x\n",
    "            y_max = (ys[i] + hs[i]) * scale_y\n",
    "            \n",
    "            # --- CRITICAL FIX: CLAMPING ---\n",
    "            # Ensure coordinates stay strictly within image boundaries [0, resize_to]\n",
    "            # We subtract 1 from the max dimension to keep it valid (0-511 for a 512 image)\n",
    "            x_min = max(0, min(x_min, self.resize_to[0] - 1))\n",
    "            y_min = max(0, min(y_min, self.resize_to[1] - 1))\n",
    "            x_max = max(0, min(x_max, self.resize_to[0]))\n",
    "            y_max = max(0, min(y_max, self.resize_to[1]))\n",
    "\n",
    "            # Only add the box if it still has area (width > 0 and height > 0)\n",
    "            if (x_max > x_min) and (y_max > y_min):\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        # 5. Create Targets\n",
    "        # Handle cases where clamping removed all boxes or original list was empty\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            area = torch.zeros((0,), dtype=torch.float32)\n",
    "        else: \n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.ones((len(boxes),), dtype=torch.int64) # Class 1 = Opacity\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = image_id\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "\n",
    "        return img_tensor, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcbaa3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faster R-CNN requires a custom collate function for batching\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79f2e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detection_ds = RSNADetectionDataset(\n",
    "    dataframe = grouped_df,\n",
    "    path_dict = train_path_dict,\n",
    "    resize_to = (512, 512)\n",
    ")\n",
    "\n",
    "train_detection_loader = DataLoader(\n",
    "    train_detection_ds,\n",
    "    batch_size = 8, \n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    "    collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532fbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faster R-CNN model initialized and moved to device.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision\n",
    "\n",
    "def get_detection_model(num_classes):\n",
    "    #load the pre-trained model Faster R-CNN \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "\n",
    "    #get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    #replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "#Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 2 #background and lung opacity\n",
    "model = get_detection_model(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Faster R-CNN model initialized and moved to device.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8fc700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred medically intelligent weights to Faster R-CNN backbone.\n",
      "Transfer details: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the weights from your classifier\n",
    "classifier_weights = final_model.image_model.state_dict()\n",
    "\n",
    "# 2. Load them into the 'body' of the detector backbone\n",
    "# CRITICAL CHANGE: Added .body before .load_state_dict\n",
    "msg = model.backbone.body.load_state_dict(classifier_weights, strict=False)\n",
    "\n",
    "print(\"Transferred medically intelligent weights to Faster R-CNN backbone.\")\n",
    "print(f\"Transfer details: {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "692d6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = optim.SGD(params,\n",
    "                      lr = 0.0005,\n",
    "                      momentum=0.9,\n",
    "                      weight_decay = 0.0005)\n",
    "\n",
    "#learning rete scheduler\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                         step_size=3,\n",
    "                                         gamma=0.1)\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_visual_check(model, dataset, idx, device, epoch_num, save_path = 'visual_check.png'):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #get the image and target\n",
    "        img_tensor, _ = dataset[idx]\n",
    "\n",
    "        #Run inference\n",
    "        prediction = model([img_tensor.to(device)])[0]\n",
    "\n",
    "        #covert tensor to PIL Image\n",
    "        pil_img = transforms.ToPILImage()(img_tensor.cpu())\n",
    "\n",
    "        #prepare drawing tool\n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "\n",
    "        #draw boxes\n",
    "        boxes = prediction['boxes'].cpu().numpy()\n",
    "        scores = prediction['scores'].cpu().numpy()\n",
    "\n",
    "        found_box = False\n",
    "\n",
    "        for i, box in enumerate(boxes):\n",
    "            score = scores[i]\n",
    "            if score >= 0.5:\n",
    "                found_box = True\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                draw.rectangle([(x_min, y_min), (x_max, y_max)], outline='red', width=3)\n",
    "                text = f\"{score:.2f}\"\n",
    "                draw.text((x_min + 2, y_min - 10), text, fill = 'white')\n",
    "    \n",
    "    final_filename = f'epoch_{epoch_num}_' + save_path\n",
    "    pil_img.save(final_filename)\n",
    "\n",
    "    status = \"Found Boxes\" if found_box else \"No Boxes Found > 0.5\"\n",
    "    print(f\"Visual check saved to: {final_filename} ({status})\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dataset(dataset):\n",
    "    print(f\"Scanning {len(dataset)} items for invalid boxes...\")\n",
    "    invalid_count = 0\n",
    "    for i in range(len(dataset)):\n",
    "        try:\n",
    "            img, target = dataset[i]\n",
    "            boxes = target['boxes']\n",
    "            \n",
    "            # Check 1: No boxes (shouldn't happen if we filtered for Opacity)\n",
    "            if boxes.shape[0] == 0:\n",
    "                print(f\"Warning: Index {i} has empty boxes.\")\n",
    "                \n",
    "            # Check 2: Negative coordinates\n",
    "            if (boxes < 0).any():\n",
    "                print(f\"FAIL: Index {i} has negative coordinates: {boxes}\")\n",
    "                invalid_count += 1\n",
    "                \n",
    "            # Check 3: x_max <= x_min\n",
    "            # boxes is [x_min, y_min, x_max, y_max]\n",
    "            widths = boxes[:, 2] - boxes[:, 0]\n",
    "            heights = boxes[:, 3] - boxes[:, 1]\n",
    "            \n",
    "            if (widths <= 0).any() or (heights <= 0).any():\n",
    "                print(f\"FAIL: Index {i} has inverted/zero-area box: {boxes}\")\n",
    "                invalid_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CRASH at Index {i}: {e}\")\n",
    "            invalid_count += 1\n",
    "            \n",
    "    if invalid_count == 0:\n",
    "        print(\"✅ Data Integrity Check Passed. All boxes are valid.\")\n",
    "    else:\n",
    "        print(f\"❌ Found {invalid_count} invalid items.\")\n",
    "\n",
    "# Run it\n",
    "verify_dataset(train_detection_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9612ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏         | 9/676 [02:20<2:53:56, 15.65s/it, loss=1886895603712.0000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#logging\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_value\n\u001b[0;32m     52\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train\n",
    "\n",
    "## Freeze batchnorm\n",
    "#this prevents the backbone statistics from fluctuating wildly with small batches\n",
    "def freeze_batchnorm_stats(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.BatchNorm2d):\n",
    "            module.eval()\n",
    "\n",
    "model.train()\n",
    "freeze_batchnorm_stats(model)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(train_detection_loader, desc = f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for images, targets in progress_bar:\n",
    "        #move data to device\n",
    "        images = list(image.to(device) for image in images)\n",
    "\n",
    "        #targets must be a list of dictionaries (one dict per images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        #zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward pass (returns loss dictionary)\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        #sum all losses\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Check for NaN immediately\n",
    "        if torch.isnan(losses):\n",
    "            print(\"ERROR: Loss went to NaN!\")\n",
    "            continue # Skip this batch to prevent crashing the whole run\n",
    "\n",
    "        #backward pass\n",
    "        losses.backward()\n",
    "\n",
    "        # This forces gradients to stay small, preventing explosions\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        #logging\n",
    "        loss_value = losses.item()\n",
    "        epoch_loss += loss_value\n",
    "        progress_bar.set_postfix(loss=f\"{loss_value: .4f}\")\n",
    "\n",
    "    #update learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {epoch_loss/len(train_detection_loader):.4f}\")\n",
    "\n",
    "    #save checkpoint\n",
    "    torch.save(model.state_dict(), f'faster_rcnn_epoch_{epoch+1}.pth')\n",
    "\n",
    "    try:\n",
    "        save_visual_check(model, train_detection_ds, idx = 0, device = device, epoch_num = epoch+1)\n",
    "    except Exception as e:\n",
    "        print(f\"Visual check failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d3fc54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING CONTROL TEST ---\n",
      "Batch 0: Loss = 1.1104\n",
      "Batch 1: Loss = 0.8452\n",
      "Batch 2: Loss = 0.5505\n",
      "Batch 3: Loss = 0.3050\n",
      "Batch 4: Loss = 0.3250\n",
      "Batch 5: Loss = 0.2132\n",
      "Batch 6: Loss = 0.2926\n",
      "Batch 7: Loss = 0.2744\n",
      "Batch 8: Loss = 0.3157\n",
      "Batch 9: Loss = 0.3188\n",
      "Batch 10: Loss = 0.3718\n",
      "Batch 11: Loss = 0.3343\n",
      "Batch 12: Loss = 0.3473\n",
      "Batch 13: Loss = 0.3905\n",
      "Batch 14: Loss = 0.3219\n",
      "Batch 15: Loss = 0.4499\n",
      "Batch 16: Loss = 0.4459\n",
      "Batch 17: Loss = 0.4141\n",
      "Batch 18: Loss = 0.3990\n",
      "Batch 19: Loss = 0.4001\n",
      "Batch 20: Loss = 0.3943\n",
      "--- CONTROL TEST FINISHED ---\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize STANDARD model (No custom weights)\n",
    "model_control = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# 2. Modify the head for 2 classes (Background + Opacity)\n",
    "in_features = model_control.roi_heads.box_predictor.cls_score.in_features\n",
    "model_control.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
    "model_control.to(device)\n",
    "\n",
    "# 3. Quick Training Check (Just 1 epoch, small LR)\n",
    "params = [p for p in model_control.parameters() if p.requires_grad]\n",
    "optimizer_control = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "print(\"--- STARTING CONTROL TEST ---\")\n",
    "model_control.train()\n",
    "\n",
    "# We only run 20 batches to check stability\n",
    "for i, (images, targets) in enumerate(train_detection_loader):\n",
    "    if i > 20: break \n",
    "    \n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    \n",
    "    optimizer_control.zero_grad()\n",
    "    loss_dict = model_control(images, targets)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "    \n",
    "    # Clip gradients just in case\n",
    "    torch.nn.utils.clip_grad_norm_(model_control.parameters(), max_norm=2.0)\n",
    "    \n",
    "    losses.backward()\n",
    "    optimizer_control.step()\n",
    "    \n",
    "    print(f\"Batch {i}: Loss = {losses.item():.4f}\")\n",
    "\n",
    "print(\"--- CONTROL TEST FINISHED ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820eb818",
   "metadata": {},
   "source": [
    "CHANGE OF PLAN <br>\n",
    "The model learned for classification is not helping Faster RCNN model with region proposals. Probably, it has not learned to detect edges as that was not the goal of its training. This has caused the loss values to explode. Continuing training for such a model is pointless. The plan therefore now, is to train Faster RCNN with standard weights and use the previouly trained classifier in a pipeline. So only if the classification is \"lung_opacity\", the image will be passed on the Faster RCNN model. The previous code cell demonstrated that using standard weights will not be too troublesome either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15da48bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detection_ds = RSNADetectionDataset(\n",
    "    dataframe=grouped_df,\n",
    "    path_dict=train_path_dict,\n",
    "    resize_to=(512, 512)\n",
    ")\n",
    "\n",
    "# Collate function for batching\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_detection_loader = DataLoader(\n",
    "    train_detection_ds,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be23486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Faster R-CNN initialized (COCO Weights).\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes = 2 #background and lung opacity\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(\"Standard Faster R-CNN initialized (COCO Weights).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46517392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = optim.SGD(params,\n",
    "                      lr = 0.001,\n",
    "                      momentum=0.9,\n",
    "                      weight_decay = 0.0005)\n",
    "\n",
    "#learning rete scheduler\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                         step_size=3,\n",
    "                                         gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bdcb388",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freeze batchnorm\n",
    "#this prevents the backbone statistics from fluctuating wildly with small batches\n",
    "def freeze_batchnorm_stats(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.BatchNorm2d):\n",
    "            module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92b258fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "def save_visual_check(model, dataset, idx, device, epoch_num, save_path = 'visual_check.png'):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #get the image and target\n",
    "        img_tensor, _ = dataset[idx]\n",
    "\n",
    "        #Run inference\n",
    "        prediction = model([img_tensor.to(device)])[0]\n",
    "\n",
    "        #covert tensor to PIL Image\n",
    "        pil_img = transforms.ToPILImage()(img_tensor.cpu())\n",
    "\n",
    "        #prepare drawing tool\n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "\n",
    "        #draw boxes\n",
    "        boxes = prediction['boxes'].cpu().numpy()\n",
    "        scores = prediction['scores'].cpu().numpy()\n",
    "\n",
    "        found_box = False\n",
    "\n",
    "        for i, box in enumerate(boxes):\n",
    "            score = scores[i]\n",
    "            if score >= 0.5:\n",
    "                found_box = True\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                draw.rectangle([(x_min, y_min), (x_max, y_max)], outline='red', width=3)\n",
    "                text = f\"{score:.2f}\"\n",
    "                draw.text((x_min + 2, y_min - 10), text, fill = 'white')\n",
    "    \n",
    "    final_filename = f'epoch_{epoch_num}_' + save_path\n",
    "    pil_img.save(final_filename)\n",
    "\n",
    "    status = \"Found Boxes\" if found_box else \"No Boxes Found > 0.5\"\n",
    "    print(f\"Visual check saved to: {final_filename} ({status})\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ef3c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING TRAINING ---\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  13%|█▎        | 87/676 [28:22<3:12:08, 19.57s/it, loss=0.3173]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Safety Check\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[1;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torchvision\\models\\detection\\rpn.py:371\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[1;34m(self, images, features, targets)\u001b[0m\n\u001b[0;32m    367\u001b[0m objectness, pred_bbox_deltas \u001b[38;5;241m=\u001b[39m concat_box_prediction_layers(objectness, pred_bbox_deltas)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m# apply pred_bbox_deltas to anchors to obtain the decoded proposals\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# note that we detach the deltas because Faster R-CNN do not backprop through\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# the proposals\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_coder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bbox_deltas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m proposals \u001b[38;5;241m=\u001b[39m proposals\u001b[38;5;241m.\u001b[39mview(num_images, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    373\u001b[0m boxes, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_proposals(proposals, objectness, images\u001b[38;5;241m.\u001b[39mimage_sizes, num_anchors_per_level)\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torchvision\\models\\detection\\_utils.py:178\u001b[0m, in \u001b[0;36mBoxCoder.decode\u001b[1;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box_sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    177\u001b[0m     rel_codes \u001b[38;5;241m=\u001b[39m rel_codes\u001b[38;5;241m.\u001b[39mreshape(box_sum, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 178\u001b[0m pred_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box_sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    180\u001b[0m     pred_boxes \u001b[38;5;241m=\u001b[39m pred_boxes\u001b[38;5;241m.\u001b[39mreshape(box_sum, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torchvision\\models\\detection\\_utils.py:216\u001b[0m, in \u001b[0;36mBoxCoder.decode_single\u001b[1;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[0;32m    213\u001b[0m pred_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(dh) \u001b[38;5;241m*\u001b[39m heights[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Distance from center to box's corner.\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m c_to_c_h \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_ctr_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_h\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m pred_h\n\u001b[0;32m    217\u001b[0m c_to_c_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mpred_ctr_x\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mpred_w\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m pred_w\n\u001b[0;32m    219\u001b[0m pred_boxes1 \u001b[38;5;241m=\u001b[39m pred_ctr_x \u001b[38;5;241m-\u001b[39m c_to_c_w\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "print(\"--- STARTING TRAINING ---\")\n",
    "model.train()\n",
    "freeze_batchnorm_stats(model) # Freeze BN stats once before starting\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(train_detection_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        # Move to device\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Safety Check\n",
    "        if not torch.isfinite(losses):\n",
    "            print(f\"WARNING: Loss is {losses.item()}, skipping batch.\")\n",
    "            continue\n",
    "            \n",
    "        # Backward pass\n",
    "        losses.backward()\n",
    "        \n",
    "        # Clip Gradients (Stability)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        loss_value = losses.item()\n",
    "        epoch_loss += loss_value\n",
    "        progress_bar.set_postfix(loss=f\"{loss_value:.4f}\")\n",
    "    \n",
    "    # End of Epoch Updates\n",
    "    lr_scheduler.step()\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {epoch_loss/len(train_detection_loader):.4f}\")\n",
    "    \n",
    "    # Save Model\n",
    "    torch.save(model.state_dict(), f'faster_rcnn_epoch_{epoch+1}.pth')\n",
    "    \n",
    "    # Visual Check\n",
    "    try:\n",
    "        save_visual_check(model, train_detection_ds, idx=0, device=device, epoch_num=epoch+1)\n",
    "    except Exception as e:\n",
    "        print(f\"Visual check failed: {e}\")\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8401cb",
   "metadata": {},
   "source": [
    "This setup is taking too long to train. <br>\n",
    "Possible solutions:\n",
    "- Pre-resize images\n",
    "- use 320x320 instead of 512x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bdf8b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-resizing training images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24015/24015 [22:54<00:00, 17.48it/s]\n"
     ]
    }
   ],
   "source": [
    "#creating new resized folders\n",
    "import os\n",
    "\n",
    "os.makedirs(\"fast_train_images\", exist_ok=True)\n",
    "\n",
    "print(\"Pre-resizing training images...\")\n",
    "for pid, path in tqdm(train_path_dict.items()):\n",
    "    #open image and resize\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = img.resize((320,320))\n",
    "    img.save(f\"fast_train_images/{pid}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5307c6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated path dict for 24015 images.\n"
     ]
    }
   ],
   "source": [
    "fast_train_path_dict = {}\n",
    "\n",
    "for pid in train_path_dict.keys():\n",
    "    # Point to the new .png file\n",
    "    fast_train_path_dict[pid] = f\"fast_train_images/{pid}.png\"\n",
    "\n",
    "print(f\"Updated path dict for {len(fast_train_path_dict)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf68e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. THE CACHED DATASET (RAM LOADING) ---\n",
    "class CachedRSNADataset(Dataset):\n",
    "    def __init__(self, dataframe, path_dict):\n",
    "        self.df = dataframe[dataframe['patientId'].isin(path_dict.keys())].reset_index(drop=True)\n",
    "        self.path_dict = path_dict\n",
    "        \n",
    "        self.current_size = (320, 320)\n",
    "        self.original_size = (1024, 1024) \n",
    "        \n",
    "        # LOAD ALL IMAGES INTO RAM NOW\n",
    "        self.images_in_ram = []\n",
    "        print(f\"Loading {len(self.df)} images into RAM (approx 1-2 mins)...\")\n",
    "        \n",
    "        for idx in tqdm(range(len(self.df))):\n",
    "            row = self.df.iloc[idx]\n",
    "            pid = row['patientId']\n",
    "            image_path = self.path_dict[pid]\n",
    "            \n",
    "            with Image.open(image_path) as img:\n",
    "                self.images_in_ram.append(img.convert(\"RGB\"))\n",
    "                \n",
    "        print(\"✅ All images loaded. Training will be fast.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Instant access from RAM\n",
    "        img = self.images_in_ram[idx]\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "\n",
    "        # Handle Boxes\n",
    "        row = self.df.iloc[idx]\n",
    "        boxes = []\n",
    "        xs = row['x']; ys = row['y']; ws = row['width']; hs = row['height']\n",
    "        \n",
    "        scale_x = self.current_size[0] / self.original_size[0]\n",
    "        scale_y = self.current_size[1] / self.original_size[1]\n",
    "\n",
    "        for i in range(len(xs)):\n",
    "            x_min = max(0, min(xs[i] * scale_x, self.current_size[0] - 1))\n",
    "            y_min = max(0, min(ys[i] * scale_y, self.current_size[1] - 1))\n",
    "            x_max = max(0, min((xs[i] + ws[i]) * scale_x, self.current_size[0]))\n",
    "            y_max = max(0, min((ys[i] + hs[i]) * scale_y, self.current_size[1]))\n",
    "\n",
    "            if (x_max > x_min) and (y_max > y_min):\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            area = torch.zeros((0,), dtype=torch.float32)\n",
    "        else: \n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.ones((len(boxes),), dtype=torch.int64)\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor([idx])\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "\n",
    "        return img_tensor, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e42de3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 5404 images into RAM (approx 1-2 mins)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5404/5404 [00:16<00:00, 336.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All images loaded. Training will be fast.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. DATALOADER ---\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Initialize Dataset\n",
    "train_detection_ds = CachedRSNADataset(\n",
    "    dataframe=grouped_df,\n",
    "    path_dict=fast_train_path_dict \n",
    ")\n",
    "\n",
    "# INCREASED BATCH SIZE TO 16 for extra speed\n",
    "train_detection_loader = DataLoader(\n",
    "    train_detection_ds,\n",
    "    batch_size=16, \n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb4b0fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING OPTIMIZED TRAINING ---\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  12%|█▏        | 40/338 [50:37<6:17:12, 75.95s/it, loss=0.3660]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 56\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(losses):\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[1;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torchvision\\models\\detection\\rpn.py:371\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[1;34m(self, images, features, targets)\u001b[0m\n\u001b[0;32m    367\u001b[0m objectness, pred_bbox_deltas \u001b[38;5;241m=\u001b[39m concat_box_prediction_layers(objectness, pred_bbox_deltas)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m# apply pred_bbox_deltas to anchors to obtain the decoded proposals\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# note that we detach the deltas because Faster R-CNN do not backprop through\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# the proposals\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_coder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bbox_deltas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m proposals \u001b[38;5;241m=\u001b[39m proposals\u001b[38;5;241m.\u001b[39mview(num_images, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    373\u001b[0m boxes, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_proposals(proposals, objectness, images\u001b[38;5;241m.\u001b[39mimage_sizes, num_anchors_per_level)\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torchvision\\models\\detection\\_utils.py:178\u001b[0m, in \u001b[0;36mBoxCoder.decode\u001b[1;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box_sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    177\u001b[0m     rel_codes \u001b[38;5;241m=\u001b[39m rel_codes\u001b[38;5;241m.\u001b[39mreshape(box_sum, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 178\u001b[0m pred_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box_sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    180\u001b[0m     pred_boxes \u001b[38;5;241m=\u001b[39m pred_boxes\u001b[38;5;241m.\u001b[39mreshape(box_sum, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Aalap\\anaconda3\\envs\\pytorch_gpu2\\lib\\site-packages\\torchvision\\models\\detection\\_utils.py:216\u001b[0m, in \u001b[0;36mBoxCoder.decode_single\u001b[1;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[0;32m    213\u001b[0m pred_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(dh) \u001b[38;5;241m*\u001b[39m heights[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Distance from center to box's corner.\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m c_to_c_h \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_ctr_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_h\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m pred_h\n\u001b[0;32m    217\u001b[0m c_to_c_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mpred_ctr_x\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mpred_w\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m pred_w\n\u001b[0;32m    219\u001b[0m pred_boxes1 \u001b[38;5;241m=\u001b[39m pred_ctr_x \u001b[38;5;241m-\u001b[39m c_to_c_w\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(model.roi_heads.box_predictor.cls_score.in_features, 2)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Helper for visualization\n",
    "def save_visual_check(model, dataset, idx, device, epoch_num, save_path='visual_check.png'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img_tensor, _ = dataset[idx]\n",
    "        prediction = model([img_tensor.to(device)])[0]\n",
    "        pil_img = transforms.ToPILImage()(img_tensor.cpu())\n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "        boxes = prediction['boxes'].cpu().numpy()\n",
    "        scores = prediction['scores'].cpu().numpy()\n",
    "        \n",
    "        found_box = False\n",
    "        for i, box in enumerate(boxes):\n",
    "            if scores[i] >= 0.5:\n",
    "                found_box = True\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                draw.rectangle([(x_min, y_min), (x_max, y_max)], outline='red', width=3)\n",
    "                draw.text((x_min, y_min - 10), f\"{scores[i]:.2f}\", fill='white')\n",
    "    \n",
    "    final_filename = f'epoch_{epoch_num}_{save_path}'\n",
    "    pil_img.save(final_filename)\n",
    "    model.train()\n",
    "\n",
    "def freeze_batchnorm_stats(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.BatchNorm2d):\n",
    "            module.eval()\n",
    "\n",
    "# --- 5. TRAINING LOOP ---\n",
    "print(\"--- STARTING OPTIMIZED TRAINING ---\")\n",
    "model.train()\n",
    "freeze_batchnorm_stats(model)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(train_detection_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        if not torch.isfinite(losses):\n",
    "            continue\n",
    "            \n",
    "        losses.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value = losses.item()\n",
    "        epoch_loss += loss_value\n",
    "        progress_bar.set_postfix(loss=f\"{loss_value:.4f}\")\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {epoch_loss/len(train_detection_loader):.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), f'faster_rcnn_epoch_{epoch+1}.pth')\n",
    "    \n",
    "    try:\n",
    "        save_visual_check(model, train_detection_ds, idx=0, device=device, epoch_num=epoch+1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbeae3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1\n",
      "CUDA Available: True\n",
      "Current Device: NVIDIA GeForce GTX 1660 Ti\n",
      "Device Count: 1\n",
      "\n",
      "--- GPU SPEED TEST ---\n",
      "Batch processing time: 1.3117 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device Count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"❌ CRITICAL: Running on CPU. This explains the slowness.\")\n",
    "\n",
    "# Speed Test\n",
    "print(\"\\n--- GPU SPEED TEST ---\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.randn(8, 3, 320, 320).to(device)\n",
    "\n",
    "start = time.time()\n",
    "# Run a dummy pass through the model's backbone\n",
    "if 'model' in locals():\n",
    "    model.to(device)\n",
    "    model.backbone(x)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    print(f\"Batch processing time: {time.time() - start:.4f} seconds\")\n",
    "else:\n",
    "    print(\"Model not loaded yet, skipping speed test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d150e1dc",
   "metadata": {},
   "source": [
    "Since the model is taking too long to train after many attempts, I am shifting the further part of the training to a new notebook to use RAM better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ea132",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
