{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10338,"databundleVersionId":862042,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T07:45:45.145150Z","iopub.execute_input":"2025-12-31T07:45:45.145453Z","iopub.status.idle":"2025-12-31T07:46:04.326082Z","shell.execute_reply.started":"2025-12-31T07:45:45.145423Z","shell.execute_reply":"2025-12-31T07:46:04.325032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#what is the size of these images\nfrom PIL import Image\nimport pydicom\n\ndicom_path = \"/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_train_images/0004cfab-14fd-4e49-80ba-63a80b6bddd6.dcm\"\nds = pydicom.dcmread(dicom_path)\n\nimage_array = ds.pixel_array\nheight, width = image_array.shape[:2]\n\nprint(f\"DICOM image size: {width}x{height}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T07:53:50.273935Z","iopub.execute_input":"2025-12-31T07:53:50.274256Z","iopub.status.idle":"2025-12-31T07:53:50.982476Z","shell.execute_reply.started":"2025-12-31T07:53:50.274235Z","shell.execute_reply":"2025-12-31T07:53:50.981095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#confirming if Target = 1 means lung_opacity and target = 0 is otherwise\nimport pandas\n\ncsv1 = pd.read_csv(\"/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_detailed_class_info.csv\")\ncsv2 = pd.read_csv(\"/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_train_labels.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T07:57:37.089978Z","iopub.execute_input":"2025-12-31T07:57:37.090866Z","iopub.status.idle":"2025-12-31T07:57:37.173024Z","shell.execute_reply.started":"2025-12-31T07:57:37.090823Z","shell.execute_reply":"2025-12-31T07:57:37.171997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"csv1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T07:57:41.964612Z","iopub.execute_input":"2025-12-31T07:57:41.965368Z","iopub.status.idle":"2025-12-31T07:57:41.994133Z","shell.execute_reply.started":"2025-12-31T07:57:41.965333Z","shell.execute_reply":"2025-12-31T07:57:41.992849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"csv2.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T07:57:46.430651Z","iopub.execute_input":"2025-12-31T07:57:46.431073Z","iopub.status.idle":"2025-12-31T07:57:46.447036Z","shell.execute_reply.started":"2025-12-31T07:57:46.431042Z","shell.execute_reply":"2025-12-31T07:57:46.446188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"combined_df = pd.merge(csv1, csv2, on = 'patientId', how = 'inner')\ncombined_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T07:59:12.654505Z","iopub.execute_input":"2025-12-31T07:59:12.654846Z","iopub.status.idle":"2025-12-31T07:59:12.715661Z","shell.execute_reply.started":"2025-12-31T07:59:12.654825Z","shell.execute_reply":"2025-12-31T07:59:12.714708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.crosstab(combined_df['class'], combined_df['Target'], margins = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T08:01:11.552816Z","iopub.execute_input":"2025-12-31T08:01:11.553174Z","iopub.status.idle":"2025-12-31T08:01:11.635077Z","shell.execute_reply.started":"2025-12-31T08:01:11.553148Z","shell.execute_reply":"2025-12-31T08:01:11.634251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers datasets accelerate scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T08:41:07.067922Z","iopub.execute_input":"2025-12-31T08:41:07.068546Z","iopub.status.idle":"2025-12-31T08:41:10.489440Z","shell.execute_reply.started":"2025-12-31T08:41:07.068513Z","shell.execute_reply":"2025-12-31T08:41:10.488493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q pydicom joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T08:41:13.623048Z","iopub.execute_input":"2025-12-31T08:41:13.623750Z","iopub.status.idle":"2025-12-31T08:41:16.686699Z","shell.execute_reply.started":"2025-12-31T08:41:13.623720Z","shell.execute_reply":"2025-12-31T08:41:16.685948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport pydicom\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom joblib import Parallel, delayed\nfrom tqdm.notebook import tqdm\n\n# ==========================================\n# 1. CONFIGURATION\n# ==========================================\n# Input Paths\nDICOM_DIR = \"/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_train_images\"\nLABELS_CSV = \"/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_train_labels.csv\"\nCLASS_INFO_CSV = \"/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_detailed_class_info.csv\"\n\n# Output Directory\nOUTPUT_DIR = \"/kaggle/working/dataset_384\"\nIMG_SIZE = 384 \n\n# ==========================================\n# 2. MERGE & MAP LABELS\n# ==========================================\nprint(\"Reading CSVs...\")\ntrain_labels = pd.read_csv(LABELS_CSV)\nclass_info = pd.read_csv(CLASS_INFO_CSV)\n\n# Merge them on patientId\n# This is crucial: We join the 'Target' from train_labels with the 'class' from class_info\ncombined_df = pd.merge(train_labels, class_info, on='patientId', how='left')\n\n# Drop Duplicates\n# The CSVs have one row per Bounding Box. For classification, we only need \n# one row per image.\ncombined_df = combined_df.drop_duplicates(subset=['patientId'])\n\nprint(f\"Total Unique Images to Process: {len(combined_df)}\")\n\n# Create Mapping: {'patient_id': 'Lung Opacity'}\nid_to_class = dict(zip(combined_df.patientId, combined_df['class']))\n\n# Create Subfolders\nunique_classes = combined_df['class'].unique()\nfor c in unique_classes:\n    safe_name = c.replace(\" \", \"_\").replace(\"/\", \"_\")\n    os.makedirs(os.path.join(OUTPUT_DIR, safe_name), exist_ok=True)\n    \nprint(f\"Created folders for: {unique_classes}\")\n\n# ==========================================\n# 3. CONVERSION WORKER\n# ==========================================\ndef process_dicom(dcm_path):\n    try:\n        patient_id = os.path.basename(dcm_path).replace('.dcm', '')\n        \n        # Look up class\n        class_name = id_to_class.get(patient_id)\n        if class_name is None:\n            # If an image is in the folder but not the CSV, we skip it\n            return \n            \n        safe_class_name = class_name.replace(\" \", \"_\").replace(\"/\", \"_\")\n        dest_path = os.path.join(OUTPUT_DIR, safe_class_name, f\"{patient_id}.jpg\")\n        \n        # Skip if already done\n        if os.path.exists(dest_path):\n            return\n\n        # Read DICOM\n        dcm = pydicom.dcmread(dcm_path)\n        img = dcm.pixel_array.astype(float)\n        \n        # Normalize (Scale to 0-255)\n        img = (img - np.min(img)) / (np.max(img) - np.min(img)) * 255.0\n        img = img.astype(np.uint8)\n        \n        # Resize (Speed up training)\n        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n        \n        # Save as JPG\n        cv2.imwrite(dest_path, img)\n        \n    except Exception as e:\n        print(f\"Error processing {patient_id}: {e}\")\n\n# ==========================================\n# 4. RUN PARALLEL PROCESSING\n# ==========================================\ndcm_files = glob.glob(os.path.join(DICOM_DIR, \"*.dcm\"))\nprint(f\"Starting conversion of {len(dcm_files)} files...\")\n\n# Use all CPU cores\nParallel(n_jobs=-1)(delayed(process_dicom)(f) for f in tqdm(dcm_files))\n\nprint(\"Processing Complete. Data ready at:\", OUTPUT_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T08:41:28.117069Z","iopub.execute_input":"2025-12-31T08:41:28.117716Z","iopub.status.idle":"2025-12-31T08:45:55.104049Z","shell.execute_reply.started":"2025-12-31T08:41:28.117684Z","shell.execute_reply":"2025-12-31T08:45:55.103345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================================================\n# CELL 2: ViT TRAINING (FIXED & OPTIMIZED)\n# ====================================================\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoImageProcessor,\n    AutoModelForImageClassification,\n    TrainingArguments,\n    Trainer,\n    DefaultDataCollator\n)\nfrom torchvision import transforms\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# --- CONFIG ---\nMODEL_CHECKPOINT = \"google/vit-base-patch16-384\"\nDATA_DIR = \"/kaggle/working/dataset_384\"\nBATCH_SIZE = 24\nNUM_EPOCHS = 15\n\n# --- 1. LOAD DATA ---\ndataset = load_dataset(\"imagefolder\", data_dir=DATA_DIR)\n# Rename 'label' to 'labels' to ensure Trainer detects it correctly\ndataset = dataset.rename_column(\"label\", \"labels\") \n\nsplits = dataset[\"train\"].train_test_split(test_size=0.15, seed=42)\ntrain_ds = splits[\"train\"]\nval_ds = splits[\"test\"]\nlabels_list = train_ds.features[\"labels\"].names\n\nprint(f\"Classes: {labels_list}\")\n\n# --- 2. CLASS WEIGHTS ---\ny_train = np.array(train_ds[\"labels\"])\nweights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weights = torch.tensor(weights, dtype=torch.float).cuda()\nprint(f\"Active Class Weights: {class_weights}\")\n\n# --- 3. TRANSFORMS (THE FIX IS HERE) ---\nimage_processor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT)\nnormalize = transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n\ntrain_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    normalize,\n])\nval_transforms = transforms.Compose([transforms.ToTensor(), normalize])\n\ndef preprocess_train(batch):\n    batch[\"pixel_values\"] = [train_transforms(x.convert(\"RGB\")) for x in batch[\"image\"]]\n    # CRITICAL FIX: Delete the raw 'image' column so the collator doesn't crash\n    del batch[\"image\"] \n    return batch\n\ndef preprocess_val(batch):\n    batch[\"pixel_values\"] = [val_transforms(x.convert(\"RGB\")) for x in batch[\"image\"]]\n    del batch[\"image\"] # CRITICAL FIX\n    return batch\n\ntrain_ds.set_transform(preprocess_train)\nval_ds.set_transform(preprocess_val)\n\n# --- 4. CUSTOM TRAINER & METRICS ---\ndef compute_metrics(pred):\n    labels_ids = pred.label_ids\n    preds_ids = pred.predictions.argmax(-1)\n    acc = (labels_ids == preds_ids).mean()\n    print(\"\\n\" + classification_report(labels_ids, preds_ids, target_names=labels_list, digits=4))\n    return {'accuracy': acc}\n\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.get(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\nmodel = AutoModelForImageClassification.from_pretrained(\n    MODEL_CHECKPOINT, num_labels=len(labels_list), \n    id2label={i: l for i, l in enumerate(labels_list)}, \n    label2id={l: i for i, l in enumerate(labels_list)}, \n    ignore_mismatched_sizes=True\n)\n\n# --- 5. RUN TRAINING ---\nargs = TrainingArguments(\n    output_dir=\"/kaggle/working/vit-384-final\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=2,\n    num_train_epochs=NUM_EPOCHS,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    fp16=True,                   \n    dataloader_num_workers=4,    \n    dataloader_pin_memory=True,\n    \n    # Corrected argument name for newer Transformers versions\n    eval_strategy=\"epoch\",      \n    save_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    \n    load_best_model_at_end=True,\n    report_to=\"none\",\n    remove_unused_columns=False\n)\n\ntrainer = WeightedTrainer(\n    model=model, args=args, train_dataset=train_ds, \n    eval_dataset=val_ds, compute_metrics=compute_metrics,\n    data_collator=DefaultDataCollator(),\n)\n\nprint(\"Starting Optimized Training...\")\ntrainer.train()\ntrainer.save_model(\"/kaggle/working/final_model_p100\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T08:58:47.844109Z","iopub.execute_input":"2025-12-31T08:58:47.845076Z","iopub.status.idle":"2025-12-31T10:41:52.047160Z","shell.execute_reply.started":"2025-12-31T08:58:47.845025Z","shell.execute_reply":"2025-12-31T10:41:52.046105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\n# List of folders we used in previous steps\nfolders_to_delete = [\n    \"/kaggle/working/final_model_p100\",         # Old ViT output\n    \"/kaggle/working/vit-384-final\",      # New ResNet output\n]\n\nprint(\"üßπ Cleaning up old checkpoints...\")\n\nfor folder in folders_to_delete:\n    if os.path.exists(folder):\n        try:\n            shutil.rmtree(folder)\n            print(f\"‚úÖ Deleted: {folder}\")\n        except Exception as e:\n            print(f\"‚ùå Error deleting {folder}: {e}\")\n    else:\n        print(f\"‚ö™ Not found (already clean): {folder}\")\n\nprint(\"‚ú® Ready for fresh training!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T10:50:31.100470Z","iopub.execute_input":"2025-12-31T10:50:31.101204Z","iopub.status.idle":"2025-12-31T10:50:31.649551Z","shell.execute_reply.started":"2025-12-31T10:50:31.101170Z","shell.execute_reply":"2025-12-31T10:50:31.648959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport torch\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoImageProcessor, AutoModelForImageClassification, \n    TrainingArguments, Trainer, DefaultDataCollator\n)\nfrom torchvision import transforms\nfrom sklearn.metrics import classification_report, recall_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# ====================================================\n# CONFIGURATION\n# ====================================================\nDATA_DIR = \"/kaggle/working/dataset_384\"\nOUTPUT_DIR = \"/kaggle/working/final_model_vit_recall\"\n\n# --- SAFETY CHECK ---\n# If your session restarted, this folder might be gone.\nif not os.path.exists(DATA_DIR):\n    raise FileNotFoundError(\n        f\"‚ùå ERROR: The folder {DATA_DIR} is missing!\\n\"\n        \"Your Kaggle session may have reset.\\n\"\n        \"Please re-run the 'Data Preparation' script to generate the 384px images again.\"\n    )\nelse:\n    print(f\"‚úÖ Data found at {DATA_DIR}. Proceeding to training...\")\n\n# ====================================================\n# ViT TRAINING (Max Macro Recall)\n# ====================================================\nprint(\"üöÄ Starting ViT Training...\")\n\n# 1. Load Data\ndataset = load_dataset(\"imagefolder\", data_dir=DATA_DIR)\ndataset = dataset.rename_column(\"label\", \"labels\") \nsplits = dataset[\"train\"].train_test_split(test_size=0.15, seed=42)\ntrain_ds = splits[\"train\"]\nval_ds = splits[\"test\"]\nlabels_list = train_ds.features[\"labels\"].names\n\n# 2. Weights & Transforms\ny_train = np.array(train_ds[\"labels\"])\nweights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weights = torch.tensor(weights, dtype=torch.float).cuda()\n\nmodel_ckpt = \"google/vit-base-patch16-384\"\nprocessor = AutoImageProcessor.from_pretrained(model_ckpt)\nnorm = transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n\ntrain_tf = transforms.Compose([\n    transforms.RandomHorizontalFlip(), transforms.RandomRotation(15),\n    transforms.ColorJitter(0.2, 0.2), transforms.ToTensor(), norm\n])\nval_tf = transforms.Compose([transforms.ToTensor(), norm])\n\ndef preprocess_train(batch):\n    batch[\"pixel_values\"] = [train_tf(x.convert(\"RGB\")) for x in batch[\"image\"]]\n    del batch[\"image\"]\n    return batch\ndef preprocess_val(batch):\n    batch[\"pixel_values\"] = [val_tf(x.convert(\"RGB\")) for x in batch[\"image\"]]\n    del batch[\"image\"]\n    return batch\n\ntrain_ds.set_transform(preprocess_train)\nval_ds.set_transform(preprocess_val)\n\n# 3. Custom Metrics (MACRO RECALL TARGET)\ndef compute_metrics_recall(pred):\n    labels_ids = pred.label_ids\n    preds_ids = pred.predictions.argmax(-1)\n    \n    # Calculate Macro Recall\n    macro_recall = recall_score(labels_ids, preds_ids, average='macro')\n    \n    print(\"\\n\" + classification_report(labels_ids, preds_ids, target_names=labels_list, digits=4))\n    \n    return {\n        'accuracy': (labels_ids == preds_ids).mean(),\n        'eval_macro_recall': macro_recall \n    }\n\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.get(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\nmodel = AutoModelForImageClassification.from_pretrained(\n    model_ckpt, num_labels=3, \n    id2label={i: l for i, l in enumerate(labels_list)}, \n    label2id={l: i for i, l in enumerate(labels_list)}, ignore_mismatched_sizes=True\n)\n\nargs = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    learning_rate=2e-5, per_device_train_batch_size=24, per_device_eval_batch_size=24,\n    gradient_accumulation_steps=2, num_train_epochs=15,\n    fp16=True, dataloader_num_workers=4, dataloader_pin_memory=True,\n    eval_strategy=\"epoch\", save_strategy=\"epoch\", \n    \n    # --- MAXIMIZE MACRO RECALL ---\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_macro_recall\", \n    greater_is_better=True,\n    save_total_limit=2,\n    # -----------------------------\n    report_to=\"none\", remove_unused_columns=False\n)\n\ntrainer = WeightedTrainer(\n    model=model, args=args, train_dataset=train_ds, \n    eval_dataset=val_ds, compute_metrics=compute_metrics_recall,\n    data_collator=DefaultDataCollator(),\n)\n\n# Start training\n# We check for existing model file to prevent accidental overwrites if you run this cell twice\nif not os.path.exists(os.path.join(OUTPUT_DIR, \"pytorch_model.bin\")):\n    trainer.train()\n    trainer.save_model(OUTPUT_DIR)\n    print(\"‚úÖ ViT Training Complete.\")\nelse:\n    print(\"‚úÖ Output already exists. Skipping training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T10:55:23.448035Z","iopub.execute_input":"2025-12-31T10:55:23.448589Z","iopub.status.idle":"2025-12-31T16:34:38.587987Z","shell.execute_reply.started":"2025-12-31T10:55:23.448562Z","shell.execute_reply":"2025-12-31T16:34:38.587218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/working/final_model_vit_recall\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:44:55.881855Z","iopub.execute_input":"2025-12-31T16:44:55.882577Z","iopub.status.idle":"2025-12-31T16:44:55.887052Z","shell.execute_reply.started":"2025-12-31T16:44:55.882541Z","shell.execute_reply":"2025-12-31T16:44:55.886463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"vit_best_model\", 'zip', \"/kaggle/working/final_model_vit_recall\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:45:43.239186Z","iopub.execute_input":"2025-12-31T16:45:43.239717Z","iopub.status.idle":"2025-12-31T16:47:38.573463Z","shell.execute_reply.started":"2025-12-31T16:45:43.239688Z","shell.execute_reply":"2025-12-31T16:47:38.572695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# This creates a clickable link to download the file directly\nFileLink(r'vit_best_model.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T16:49:32.225858Z","iopub.execute_input":"2025-12-31T16:49:32.226635Z","iopub.status.idle":"2025-12-31T16:49:32.231300Z","shell.execute_reply.started":"2025-12-31T16:49:32.226604Z","shell.execute_reply":"2025-12-31T16:49:32.230618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport torch\nimport glob\nimport cv2\nimport shutil\nimport pandas as pd\nimport numpy as np\nimport pydicom\nfrom joblib import Parallel, delayed\nfrom tqdm.notebook import tqdm\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoImageProcessor, AutoModelForImageClassification, \n    TrainingArguments, Trainer, DefaultDataCollator\n)\nfrom torchvision import transforms\nfrom sklearn.metrics import classification_report, recall_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# ====================================================\n# CONFIGURATION\n# ====================================================\nRESNET_OUTPUT_DIR = \"/kaggle/working/final_model_resnet_recall\"\nDATA_DIR_640 = \"/kaggle/working/dataset_640\"\nMODEL_CHECKPOINT = \"microsoft/resnet-101\"\n\n# ====================================================\n# 1. DATA GENERATION (Run if 640px folder is missing)\n# ====================================================\ndef prepare_data_640():\n    if os.path.exists(DATA_DIR_640):\n        print(f\"‚úÖ Data found at {DATA_DIR_640}. Skipping generation.\")\n        return\n\n    print(\"‚öôÔ∏è Generating 640px Dataset (ResNet requires High Res)...\")\n    DICOM_DIR = \"/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_train_images\"\n    LABELS_CSV = \"/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_train_labels.csv\"\n    CLASS_INFO_CSV = \"/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_detailed_class_info.csv\"\n    \n    # Merge & Map\n    train_labels = pd.read_csv(LABELS_CSV)\n    class_info = pd.read_csv(CLASS_INFO_CSV)\n    combined = pd.merge(train_labels, class_info, on='patientId', how='left').drop_duplicates(subset=['patientId'])\n    id_to_class = dict(zip(combined.patientId, combined['class']))\n    \n    # Create Folders\n    for c in combined['class'].unique():\n        safe_name = c.replace(\" \", \"_\").replace(\"/\", \"_\")\n        os.makedirs(os.path.join(DATA_DIR_640, safe_name), exist_ok=True)\n        \n    # Parallel Convert\n    def process(dcm_path):\n        try:\n            pid = os.path.basename(dcm_path).replace('.dcm', '')\n            cname = id_to_class.get(pid)\n            if cname:\n                safe_cname = cname.replace(\" \", \"_\").replace(\"/\", \"_\")\n                save_path = os.path.join(DATA_DIR_640, safe_cname, f\"{pid}.jpg\")\n                if not os.path.exists(save_path):\n                    dcm = pydicom.dcmread(dcm_path)\n                    img = dcm.pixel_array.astype(float)\n                    img = (img - np.min(img)) / (np.max(img) - np.min(img)) * 255.0\n                    img = cv2.resize(img.astype(np.uint8), (640, 640))\n                    cv2.imwrite(save_path, img)\n        except: pass\n\n    files = glob.glob(os.path.join(DICOM_DIR, \"*.dcm\"))\n    Parallel(n_jobs=-1)(delayed(process)(f) for f in tqdm(files))\n    print(\"‚úÖ Generated 640px Data.\")\n\nprepare_data_640()\n\n# ====================================================\n# 2. RESNET TRAINING (Max Macro Recall)\n# ====================================================\nprint(\"\\nüöÄ Starting ResNet-101 Training...\")\n\n# Load Data\ndataset = load_dataset(\"imagefolder\", data_dir=DATA_DIR_640)\ndataset = dataset.rename_column(\"label\", \"labels\") \nsplits = dataset[\"train\"].train_test_split(test_size=0.15, seed=42)\ntrain_ds = splits[\"train\"]\nval_ds = splits[\"test\"]\nlabels_list = train_ds.features[\"labels\"].names\n\n# Class Weights\ny_train = np.array(train_ds[\"labels\"])\nweights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weights = torch.tensor(weights, dtype=torch.float).cuda()\n\n# Transforms (ResNet Standard)\nprocessor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n# Use ImageNet stats if processor doesn't have them\nmean = processor.image_mean if hasattr(processor, 'image_mean') else [0.485, 0.456, 0.406]\nstd = processor.image_std if hasattr(processor, 'image_std') else [0.229, 0.224, 0.225]\nnorm = transforms.Normalize(mean=mean, std=std)\n\ntrain_tf = transforms.Compose([\n    transforms.RandomHorizontalFlip(), transforms.RandomRotation(10),\n    transforms.ColorJitter(0.1, 0.1), transforms.ToTensor(), norm\n])\nval_tf = transforms.Compose([transforms.ToTensor(), norm])\n\ndef preprocess_train(batch):\n    batch[\"pixel_values\"] = [train_tf(x.convert(\"RGB\")) for x in batch[\"image\"]]\n    del batch[\"image\"]\n    return batch\ndef preprocess_val(batch):\n    batch[\"pixel_values\"] = [val_tf(x.convert(\"RGB\")) for x in batch[\"image\"]]\n    del batch[\"image\"]\n    return batch\n\ntrain_ds.set_transform(preprocess_train)\nval_ds.set_transform(preprocess_val)\n\n# Metrics (Macro Recall)\ndef compute_metrics_recall(pred):\n    labels_ids = pred.label_ids\n    preds_ids = pred.predictions.argmax(-1)\n    macro_recall = recall_score(labels_ids, preds_ids, average='macro')\n    print(\"\\n\" + classification_report(labels_ids, preds_ids, target_names=labels_list, digits=4))\n    return {'accuracy': (labels_ids == preds_ids).mean(), 'eval_macro_recall': macro_recall}\n\n# Custom Trainer\nclass WeightedTrainerResNet(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.get(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\nmodel = AutoModelForImageClassification.from_pretrained(\n    MODEL_CHECKPOINT, num_labels=3, \n    id2label={i: l for i, l in enumerate(labels_list)}, \n    label2id={l: i for i, l in enumerate(labels_list)}, ignore_mismatched_sizes=True\n)\n\n# Arguments (Optimized for P100 @ 640px)\nargs = TrainingArguments(\n    output_dir=RESNET_OUTPUT_DIR,\n    learning_rate=2e-5, \n    per_device_train_batch_size=12,   # Smaller batch for 640px\n    per_device_eval_batch_size=12,\n    gradient_accumulation_steps=4,    # Accumulate to reach effective batch ~48\n    num_train_epochs=15,\n    fp16=True, \n    dataloader_num_workers=4, \n    dataloader_pin_memory=True,\n    eval_strategy=\"epoch\", \n    save_strategy=\"epoch\", \n    \n    # MAXIMIZE RECALL\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_macro_recall\", \n    greater_is_better=True,\n    save_total_limit=2,\n    \n    report_to=\"none\", remove_unused_columns=False\n)\n\ntrainer = WeightedTrainerResNet(\n    model=model, args=args, train_dataset=train_ds, \n    eval_dataset=val_ds, compute_metrics=compute_metrics_recall,\n    data_collator=DefaultDataCollator(),\n)\n\n# Train & Save\ntrainer.train()\ntrainer.save_model(RESNET_OUTPUT_DIR)\n\n# Zip Immediately\nprint(\"‚úÖ Training Complete. Zipping model...\")\nshutil.make_archive(\"resnet_best_model\", 'zip', RESNET_OUTPUT_DIR)\nprint(\"üéâ Done! Download 'resnet_best_model.zip' from Output.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T03:48:12.206702Z","iopub.execute_input":"2026-01-01T03:48:12.207352Z","iopub.status.idle":"2026-01-01T09:38:01.101002Z","shell.execute_reply.started":"2026-01-01T03:48:12.207318Z","shell.execute_reply":"2026-01-01T09:38:01.100072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# This creates a clickable link to download the file directly\nFileLink(r'resnet_best_model.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:38:37.807865Z","iopub.execute_input":"2026-01-01T09:38:37.808590Z","iopub.status.idle":"2026-01-01T09:38:37.813250Z","shell.execute_reply.started":"2026-01-01T09:38:37.808564Z","shell.execute_reply":"2026-01-01T09:38:37.812547Z"}},"outputs":[],"execution_count":null}]}